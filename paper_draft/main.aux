\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{park2024linear}
\citation{elhage2022superposition}
\citation{park2024linear}
\citation{feucht2024token}
\citation{ormerod2024kitchen}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{park2024linear}
\citation{gurnee2023language}
\citation{merullo2023language}
\citation{elhage2022superposition}
\citation{scherlis2022polysemanticity}
\citation{cunningham2023sparse,gao2024scaling,rajamanoharan2024gated}
\citation{lieberum2024gemma,he2024llama}
\citation{chanin2024absorption}
\citation{chanin2025feature}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\citation{feucht2024token}
\citation{geva2022transformer}
\citation{ormerod2024kitchen}
\citation{aljaafari2024interpreting}
\citation{garcia2021probing}
\citation{dankers2022can}
\citation{radford2019language}
\citation{nanda2022transformerlens}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Representative compounds from our dataset with compositionality ratings (1=idiomatic, 5=transparent). Full dataset contains 19 compounds. All compounds tokenize as exactly two tokens in \textsc  {GPT-2}\xspace  's BPE vocabulary.\relax }}{3}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:compounds}{{1}{3}{Representative compounds from our dataset with compositionality ratings (1=idiomatic, 5=transparent). Full dataset contains 19 compounds. All compounds tokenize as exactly two tokens in \gpttwo 's BPE vocabulary.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset Construction}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:dataset}{{3.1}{3}{Dataset Construction}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experiment 1: Next-Token Prediction Analysis}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:exp1}{{3.2}{4}{Experiment 1: Next-Token Prediction Analysis}{subsection.3.2}{}}
\newlabel{eq:boost}{{1}{4}{Experiment 1: Next-Token Prediction Analysis}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiment 2: Residual Stream Direction Analysis}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:exp2}{{3.3}{4}{Experiment 2: Residual Stream Direction Analysis}{subsection.3.3}{}}
\newlabel{eq:reconstruction}{{2}{4}{Experiment 2: Residual Stream Direction Analysis}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiment 3: Layer-wise Probing}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:exp3}{{3.4}{4}{Experiment 3: Layer-wise Probing}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Experiment 4: Attention Pattern Analysis}{4}{subsection.3.5}\protected@file@percent }
\newlabel{sec:exp4}{{3.5}{4}{Experiment 4: Attention Pattern Analysis}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Next-Token Prediction Boosting}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:results_exp1}{{4.1}{4}{Next-Token Prediction Boosting}{subsection.4.1}{}}
\citation{feucht2024token}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Next-token prediction results for selected compounds. The boost ratio (equation\nobreakspace  {}\ref  {eq:boost}) measures how much more likely $w_2$ is after $w_1$ compared to a control word. Strong compounds show boost ratios of $28$--$7{,}233\times $, while single-token compounds (snowman, sunflower) show ratios below 1.\relax }}{5}{table.caption.3}\protected@file@percent }
\newlabel{tab:next_token}{{2}{5}{Next-token prediction results for selected compounds. The boost ratio (\eqref {eq:boost}) measures how much more likely $\wtwo $ is after $\wone $ compared to a control word. Strong compounds show boost ratios of $28$--$7{,}233\times $, while single-token compounds (snowman, sunflower) show ratios below 1.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Compound Directions Are 93.7\% Reconstructable}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:results_exp2}{{4.2}{5}{Compound Directions Are 93.7\% Reconstructable}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}No Token Erasure in \textsc  {GPT-2}\xspace  }{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:results_exp3}{{4.3}{5}{No Token Erasure in \gpttwo }{subsection.4.3}{}}
\citation{aljaafari2024interpreting}
\citation{dankers2022can}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Linear reconstruction of compound directions from constituent directions at the final layer. $R^2$ measures the fraction of variance explained by the linear model in equation\nobreakspace  {}\ref  {eq:reconstruction}. Residual is the normalized reconstruction error. Comp.\ is the compositionality rating (1=idiomatic, 5=transparent). More compositional compounds have higher $R^2$. \relax }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:reconstruction}{{3}{6}{Linear reconstruction of compound directions from constituent directions at the final layer. $\Rsq $ measures the fraction of variance explained by the linear model in \eqref {eq:reconstruction}. Residual is the normalized reconstruction error. Comp.\ is the compositionality rating (1=idiomatic, 5=transparent). More compositional compounds have higher $\Rsq $. \relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Probing accuracy at selected layers. Probe 1 tests whether $w_1$ identity can be recovered from the $w_2$ position (perfect accuracy = no erasure). Probe 2 tests whether compound versus control context can be distinguished. Peak compound detection accuracy is {\bf  92.2\%} at layer 8.\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab:probing}{{4}{6}{Probing accuracy at selected layers. Probe 1 tests whether $\wone $ identity can be recovered from the $\wtwo $ position (perfect accuracy = no erasure). Probe 2 tests whether compound versus control context can be distinguished. Peak compound detection accuracy is {\bf 92.2\%} at layer 8.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Attention Patterns}{6}{subsection.4.4}\protected@file@percent }
\newlabel{sec:results_exp4}{{4.4}{6}{Attention Patterns}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Validation on \textsc  {GPT-2-Medium}\xspace  }{6}{subsection.4.5}\protected@file@percent }
\newlabel{sec:validation}{{4.5}{6}{Validation on \gptmed }{subsection.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{6}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Compound Concepts Are Compositionally Derived}{6}{subsection.5.1}\protected@file@percent }
\citation{feucht2024token}
\citation{feucht2024token}
\citation{geva2022transformer}
\citation{chanin2024absorption}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Validation on \textsc  {GPT-2-Medium}\xspace  . Next-token prediction boost ratios and $R^2$ reconstruction scores are comparable or stronger than \textsc  {GPT-2}\xspace  , confirming that compositional storage is robust to model scale.\relax }}{7}{table.caption.6}\protected@file@percent }
\newlabel{tab:validation}{{5}{7}{Validation on \gptmed . Next-token prediction boost ratios and $\Rsq $ reconstruction scores are comparable or stronger than \gpttwo , confirming that compositional storage is robust to model scale.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The Role of Compositionality}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}No Token Erasure: A Model-Scale Effect?}{7}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}The U-Shaped $R^2$ Pattern}{7}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Implications}{7}{subsection.5.5}\protected@file@percent }
\citation{park2024linear}
\citation{feucht2024token}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{aljaafari2024interpreting}{{1}{2024}{{Aljaafari et~al.}}{{Aljaafari, Belinkov, and Barez}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Limitations}{8}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{8}{Conclusion}{section.6}{}}
\bibcite{chanin2024absorption}{{2}{2024}{{Chanin et~al.}}{{Chanin, Barez, Karvonen, and Nanda}}}
\bibcite{chanin2025feature}{{3}{2025}{{Chanin et~al.}}{{Chanin, Barez, and Karvonen}}}
\bibcite{cunningham2023sparse}{{4}{2023}{{Cunningham et~al.}}{{Cunningham, Ewart, Riggs, Huben, and Sharkey}}}
\bibcite{dankers2022can}{{5}{2022}{{Dankers et~al.}}{{Dankers, Lucas, and Titov}}}
\bibcite{elhage2022superposition}{{6}{2022}{{Elhage et~al.}}{{Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.}}}
\bibcite{feucht2024token}{{7}{2024}{{Feucht et~al.}}{{Feucht, Torn{\'e}, and Bau}}}
\bibcite{gao2024scaling}{{8}{2024}{{Gao et~al.}}{{Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}}}
\bibcite{garcia2021probing}{{9}{2021}{{Garcia et~al.}}{{Garcia, Kramer~Vieira, Scarton, Idiart, and Villavicencio}}}
\bibcite{geva2022transformer}{{10}{2022}{{Geva et~al.}}{{Geva, Caciularu, Wang, and Goldberg}}}
\bibcite{gurnee2023language}{{11}{2023}{{Gurnee and Tegmark}}{{}}}
\bibcite{he2024llama}{{12}{2024}{{He et~al.}}{{He, Ge, Chen, and Liu}}}
\bibcite{lieberum2024gemma}{{13}{2024}{{Lieberum et~al.}}{{Lieberum, Rajamanoharan, Conmy, Smith, Sonnerat, Varma, Kram{\'a}r, Dragan, Shah, and Nanda}}}
\bibcite{merullo2023language}{{14}{2023}{{Merullo et~al.}}{{Merullo, Eickhoff, and Pavlick}}}
\bibcite{nanda2022transformerlens}{{15}{2022}{{Nanda and Bloom}}{{}}}
\bibcite{ormerod2024kitchen}{{16}{2024}{{Ormerod et~al.}}{{Ormerod, Maldonado, and Klinger}}}
\bibcite{park2024linear}{{17}{2024}{{Park et~al.}}{{Park, Choe, and Veitch}}}
\bibcite{radford2019language}{{18}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{rajamanoharan2024gated}{{19}{2024}{{Rajamanoharan et~al.}}{{Rajamanoharan, Conmy, Smith, Lieberum, Varma, Kram{\'a}r, Shah, and Nanda}}}
\bibcite{scherlis2022polysemanticity}{{20}{2022}{{Scherlis et~al.}}{{Scherlis, Sachan, Jermyn, Benton, and Shlegeris}}}
