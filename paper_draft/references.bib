@article{park2024linear,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2024}
}

@article{elhage2022superposition,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{feucht2024token,
  title={Token Erasure as a Footprint of Implicit Vocabulary Items in {LLMs}},
  author={Feucht, Sheridan and Torn{\'e}, David and Bau, David},
  journal={arXiv preprint arXiv:2406.20086},
  year={2024}
}

@article{ormerod2024kitchen,
  title={How Is a ``Kitchen Chair'' like a ``Farm Horse''? Exploring the Representation of Compound Nominals in Transformer-Based Language Models},
  author={Ormerod, Mark and Maldonado, Jes{\'u}s and Klinger, Roman},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  year={2024}
}

@article{chanin2024absorption,
  title={A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders},
  author={Chanin, David and Barez, Fazl and Karvonen, Adamk and Nanda, Neel},
  journal={arXiv preprint arXiv:2409.14507},
  year={2024}
}

@article{geva2022transformer,
  title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author={Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2203.14680},
  year={2022}
}

@article{aljaafari2024interpreting,
  title={Interpreting Token Compositionality in {LLMs}: A Robustness Analysis},
  author={Aljaafari, Nura and Belinkov, Yonatan and Barez, Fazl},
  journal={arXiv preprint arXiv:2410.12924},
  year={2024}
}

@inproceedings{garcia2021probing,
  title={Probing for Idiomaticity in Vector Space Models},
  author={Garcia, Marcos and Kramer Vieira, Tiago and Scarton, Carolina and Idiart, Marco and Villavicencio, Aline},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2021}
}

@article{cunningham2023sparse,
  title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{merullo2023language,
  title={Language Models Implement Simple Word2Vec-style Vector Arithmetic},
  author={Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2305.16130},
  year={2023}
}

@article{gurnee2023language,
  title={Language Models Represent Space and Time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{scherlis2022polysemanticity,
  title={Polysemanticity and Capacity in Neural Networks},
  author={Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck},
  journal={arXiv preprint arXiv:2210.01892},
  year={2022}
}

@article{gao2024scaling,
  title={Scaling and Evaluating Sparse Autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{lieberum2024gemma,
  title={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on {Gemma} 2},
  author={Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2408.05147},
  year={2024}
}

@article{he2024llama,
  title={Llama Scope: Extracting Millions of Features from {Llama-3.1-8B} with Sparse Autoencoders},
  author={He, Zhengfu and Ge, Wentao and Chen, Muhao and Liu, Zhuowan},
  journal={arXiv preprint arXiv:2410.20526},
  year={2024}
}

@article{arora2016linear,
  title={Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018}
}

@article{dankers2022can,
  title={Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation},
  author={Dankers, Verna and Lucas, Christopher G and Titov, Ivan},
  journal={arXiv preprint arXiv:2205.15301},
  year={2022}
}

@article{chanin2025feature,
  title={Feature Hedging: On the Role of Feature Redundancy in Sparse Autoencoders},
  author={Chanin, David and Barez, Fazl and Karvonen, Adam},
  journal={arXiv preprint arXiv:2503.01370},
  year={2025}
}

@article{engels2024dark,
  title={Decomposing the Dark Matter of Sparse Autoencoders},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2410.14670},
  year={2024}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2019}
}

@article{nanda2022transformerlens,
  title={{TransformerLens}},
  author={Nanda, Neel and Bloom, Joseph},
  year={2022},
  note={\url{https://github.com/neelnanda-io/TransformerLens}}
}

@article{hernandez2023linearity,
  title={Linearity of Relation Decoding in Transformer Language Models},
  author={Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2308.09124},
  year={2023}
}

@article{turner2023steering,
  title={Activation Addition: Steering Language Models Without Optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and Barez, Fazl},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}

@article{rajamanoharan2024gated,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.16014},
  year={2024}
}
