\section{Methodology}
\label{sec:methodology}

We conduct four experiments on \gpttwo~\citep{radford2019language} (124M parameters, 12 layers, $\dmodel = 768$), accessing internal activations via \transformerlens~\citep{nanda2022transformerlens}.
We validate key findings on \gptmed (355M parameters, 24 layers, $\dmodel = 1024$).

\subsection{Dataset Construction}
\label{sec:dataset}

We construct a dataset of 19 compound nouns spanning the compositionality spectrum, each paired with a control phrase that preserves the second word ($\wtwo$) but replaces the first word ($\wone$) with a non-compound modifier.
\tabref{tab:compounds} shows representative examples.
Compositionality ratings range from 1 (fully idiomatic, \eg ``hot dog'') to 5 (fully transparent, \eg ``coffee table'').

\begin{table}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llllc@{}}
        \toprule
        \textbf{Compound} & \textbf{$\wone$} & \textbf{$\wtwo$} & \textbf{Control} & \textbf{Comp.} \\
        \midrule
        washing machine & washing & machine & red machine & 4 \\
        hot dog & hot & dog & big dog & 1 \\
        coffee table & coffee & table & wooden table & 5 \\
        swimming pool & swimming & pool & deep pool & 4 \\
        parking lot & parking & lot & large lot & 4 \\
        living room & living & room & large room & 4 \\
        shooting star & shooting & star & bright star & 3 \\
        chocolate cake & chocolate & cake & large cake & 5 \\
        steel bridge & steel & bridge & old bridge & 5 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Representative compounds from our dataset with compositionality ratings (1=idiomatic, 5=transparent). Full dataset contains 19 compounds. All compounds tokenize as exactly two tokens in \gpttwo's BPE vocabulary.}
    \label{tab:compounds}
\end{table}

All compounds are verified to tokenize as exactly two tokens in \gpttwo's BPE vocabulary.
Two potential compounds were excluded: ``blueberry'' (``berry'' is multi-token) and ``guinea pig'' from direction analysis (``guinea'' is multi-token in some contexts).
Each compound is embedded in 8 diverse sentence templates (\eg ``The \{compound\} was,'' ``She bought a \{compound\} for'') to ensure context diversity.
For isolation baselines, each constituent word appears in 4 templates without its compound partner.

\subsection{Experiment 1: Next-Token Prediction Analysis}
\label{sec:exp1}

For each compound $(\wone, \wtwo)$, we measure how strongly $\wone$ predicts $\wtwo$ compared to a control word.
Specifically, we compute:
\begin{equation}
    \text{Boost} = \frac{P(\wtwo \mid \text{context} + \wone)}{P(\wtwo \mid \text{context} + \wone^{\text{ctrl}})}
    \label{eq:boost}
\end{equation}
where $\wone^{\text{ctrl}}$ is the control word (\eg ``red'' for ``washing machine'').
We average probabilities across 8 sentence templates and also record the rank of $\wtwo$ among all vocabulary predictions.

\subsection{Experiment 2: Residual Stream Direction Analysis}
\label{sec:exp2}

We test whether the compound direction can be linearly reconstructed from constituent directions.
At each layer $\ell$, we collect:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item $\vh^{(\ell)}_{\text{compound}}$: the mean hidden state at the $\wtwo$ position across 8 compound contexts.
    \item $\vh^{(\ell)}_{\wone}$: the mean hidden state for $\wone$ in isolation (4 contexts).
    \item $\vh^{(\ell)}_{\wtwo}$: the mean hidden state for $\wtwo$ in isolation (4 contexts).
\end{itemize}

We fit the linear reconstruction:
\begin{equation}
    \vh^{(\ell)}_{\text{compound}} = \alpha \cdot \vh^{(\ell)}_{\wone} + \beta \cdot \vh^{(\ell)}_{\wtwo}
    \label{eq:reconstruction}
\end{equation}
using ordinary least squares, and report the coefficient of determination $\Rsq$.
We also compute cosine similarities between compound and constituent directions, and the residual norm ratio $\|\vh_{\text{compound}} - \hat{\vh}_{\text{compound}}\| / \|\vh_{\text{compound}}\|$, which measures the fraction of the compound representation that is unique (\ie not explained by constituents).

\subsection{Experiment 3: Layer-wise Probing}
\label{sec:exp3}

We train two linear probes (logistic regression, $C = 1.0$, 5-fold cross-validation) on hidden states at the $\wtwo$ position:

\para{Probe 1 (Token erasure).}
Predicts the identity of $\wone$ from the hidden state at the $\wtwo$ position.
If the model erases constituent information to form compound representations, probe accuracy should decrease in intermediate layers.

\para{Probe 2 (Compound detection).}
Classifies whether the context is a compound (\eg ``washing machine'') or a control phrase (\eg ``red machine'').
This measures how much compound-specific information is present at each layer.
We use 120 compound samples and 136 control samples.

\subsection{Experiment 4: Attention Pattern Analysis}
\label{sec:exp4}

We extract attention weights at the $\wtwo$ position and compare how much $\wtwo$ attends to $\wone$ in compound contexts versus how much $\wtwo$ attends to the preceding word in control contexts.
We analyze this across all layers and attention heads.
