# Consolidated Paper List: Where is Washing Machine stored in LLMs?

Research Question: Investigating whether compound concepts like 'washing machine' are stored as unique directions in the residual stream of LLMs, or whether the model stores 'washing' and relies on context to predict 'machine'.

Total papers: 169 (relevance score >= 2)

## Summary by Relevance Assessment

- HIGH relevance: 88 papers
- MEDIUM relevance: 80 papers
- LOW relevance: 1 papers

## Papers Ranked by Relevance

| # | Title | Authors | Year | Citations | Search Score | My Assessment | Abstract (first 2 sentences) | URL |
|---|-------|---------|------|-----------|--------------|---------------|------------------------------|-----|
| 1 | Sparse Autoencoders Find Highly Interpretable Features in Language Models | Hoagy Cunningham et al. | 2023 | 838 | 3 | HIGH | One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. | [Link](https://api.semanticscholar.org/CorpusId:261934663) |
| 2 | Toy Models of Superposition | Nelson Elhage et al. | 2022 | 595 | 2 | HIGH | Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in"superposition."We demonstrate the existence of a phase change, a surprising... | [Link](https://www.semanticscholar.org/paper/9d125f45b1d2dea01f05281470bc08e12b6c7cba) |
| 3 | The Linear Representation Hypothesis and the Geometry of Large Language Models | Kiho Park et al. | 2023 | 352 | 3 | HIGH | Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does"linear representation"actually mean? | [Link](https://api.semanticscholar.org/CorpusId:265042984) |
| 4 | Scaling and evaluating sparse autoencoders | Leo Gao et al. | 2024 | 313 | 3 | HIGH | Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. | [Link](https://api.semanticscholar.org/CorpusId:270286001) |
| 5 | Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 | Tom Lieberum et al. | 2024 | 239 | 3 | HIGH | Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse decomposition of a neural network’s latent representations into seemingly interpretable features.Despite recent excitement about their potential, research applications outside of industry are limited by the high cost of training a comprehensive suite of SAEs.In this work, we introduce Gemma Scope, an open suite of JumpR... | [Link](https://api.semanticscholar.org/CorpusId:271843380) |
| 6 | Improving Dictionary Learning with Gated Sparse Autoencoders | Senthooran Rajamanoharan et al. | 2024 | 135 | 3 | HIGH | Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. | [Link](https://www.semanticscholar.org/paper/bbf218fd97d3c18801eb6eb09345bb38e7bcc871) |
| 7 | Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors | Zeyu Yun et al. | 2021 | 112 | 2 | HIGH | Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. | [Link](https://www.semanticscholar.org/paper/7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d) |
| 8 | Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders | Zhengfu He et al. | 2024 | 86 | 3 | HIGH | Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. | [Link](https://api.semanticscholar.org/CorpusId:273654879) |
| 9 | A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders | David Chanin et al. | 2024 | 80 | 3 | HIGH | Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features ("math"may split into"algebra","geometry", etc.), a phenomenon referred to as feature splitting. | [Link](https://api.semanticscholar.org/CorpusId:272827216) |
| 10 | Automatically Interpreting Millions of Features in Large Language Models | Gonccalo Paulo et al. | 2024 | 58 | 2 | HIGH | While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each... | [Link](https://www.semanticscholar.org/paper/04d3e91a32d424f8bc076ca632c444193166fb54) |
| 11 | BatchTopK Sparse Autoencoders | Bart Bussmann et al. | 2024 | 57 | 3 | HIGH | Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. | [Link](https://api.semanticscholar.org/CorpusId:274597122) |
| 12 | Learning Multi-Level Features with Matryoshka Sparse Autoencoders | Bart Bussmann et al. | 2025 | 56 | 3 | HIGH | Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. | [Link](https://api.semanticscholar.org/CorpusId:277271553) |
| 13 | Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models | Adam Karvonen et al. | 2024 | 47 | 3 | HIGH | What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. | [Link](https://www.semanticscholar.org/paper/b244b80d0a2d36412b56c0156532d9cbeb298ffa) |
| 14 | Linear Spaces of Meanings: Compositional Structures in Vision-Language Models | Matthew Trager et al. | 2023 | 47 | 2 | HIGH | We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a preexisting vocabulary. | [Link](https://www.semanticscholar.org/paper/b6d03e92733112a5115e9a283abe6224584718d5) |
| 15 | Interpreting Attention Layer Outputs with Sparse Autoencoders | Connor Kissane et al. | 2024 | 38 | 3 | HIGH | Decomposing model activations into interpretable components is a key open problem in mechanistic interpretability. Sparse autoencoders (SAEs) are a popular method for decomposing the internal activations of trained transformers into sparse, interpretable features, and have been applied to MLP layers and the residual stream. | [Link](https://api.semanticscholar.org/CorpusId:270711110) |
| 16 | A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models | Dong Shu et al. | 2025 | 33 | 3 | HIGH | Large Language Models (LLMs) have transformed natural language processing, yet their internal mechanisms remain largely opaque. Recently, mechanistic interpretability has attracted significant attention from the research community as a means to understand the inner workings of LLMs. | [Link](https://api.semanticscholar.org/CorpusId:276885338) |
| 17 | Interpreting Neural Networks through the Polytope Lens | Sid Black et al. | 2022 | 33 | 2 | HIGH | Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? | [Link](https://api.semanticscholar.org/CorpusId:253761579) |
| 18 | Decomposing The Dark Matter of Sparse Autoencoders | Joshua Engels et al. | 2024 | 32 | 2 | HIGH | Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in"dark matter": unexplained variance in activations. | [Link](https://www.semanticscholar.org/paper/74485409331b414368616c5acdcaced4f1b4506b) |
| 19 | The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision | Liv Gorton | 2024 | 29 | 3 | HIGH | Recent work on sparse autoencoders (SAEs) has shown promise in extracting interpretable features from neural networks and addressing challenges with polysemantic neurons caused by superposition. In this paper, we apply SAEs to the early vision layers of InceptionV1, a well-studied convolutional neural network, with a focus on curve detectors. | [Link](https://www.semanticscholar.org/paper/489aa8fd55a4262fe0fddc6927d52b3ca93a59f5) |
| 20 | Efficient Dictionary Learning with Switch Sparse Autoencoders | Anish Mudide et al. | 2024 | 29 | 3 | HIGH | Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. | [Link](https://www.semanticscholar.org/paper/7713145541360109564a0c76926a5a8b279ad6e6) |
| 21 | Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders | Senthooran Rajamanoharan et al. | 2024 | 24 | 3 | HIGH | Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models’ (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. | [Link](https://www.semanticscholar.org/paper/39b391659bf214e155d77c8090f513d36706ec10) |
| 22 | Linear Representations of Political Perspective Emerge in Large Language Models | Junsol Kim et al. | 2025 | 21 | 3 | HIGH | Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. | [Link](https://www.semanticscholar.org/paper/7aac9e9c109f4684a169aea8883f54cecbb55a9a) |
| 23 | An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation | Ahmed Abdulaal et al. | 2024 | 19 | 3 | HIGH | Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision-Language Models (VLMs) suffer from hallucinations, lack interpretability, and require expensive fine-tuning. | [Link](https://api.semanticscholar.org/CorpusId:273163152) |
| 24 | Route Sparse Autoencoder to Interpret Large Language Models | Wei Shi et al. | 2025 | 17 | 3 | HIGH | Mechanistic interpretability of large language models (LLMs) aims to uncover the internal processes of information propagation and reasoning. Sparse autoencoders (SAEs) have demonstrated promise in this domain by extracting interpretable and monosemantic features. | [Link](https://api.semanticscholar.org/CorpusId:276928474) |
| 25 | Interpreting CLIP with Hierarchical Sparse Autoencoders | Vladimir Zaigrajew et al. | 2025 | 16 | 2 | HIGH | Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern s... | [Link](https://www.semanticscholar.org/paper/90f879e665a8375b32361b0a18898558b790b6c2) |
| 26 | One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models | Viacheslav Surkov et al. | 2024 | 14 | 3 | HIGH | For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. | [Link](https://api.semanticscholar.org/CorpusId:273696029) |
| 27 | Disentangling Dense Embeddings with Sparse Autoencoders | Charles O'Neill et al. | 2024 | 14 | 3 | HIGH | Sparse autoencoders (SAEs) have shown promise in extracting interpretable features from complex neural networks. We present one of the first applications of SAEs to dense text embeddings from large language models, demonstrating their effectiveness in disentangling semantic concepts. | [Link](https://api.semanticscholar.org/CorpusId:271601116) |
| 28 | Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders | Michael Lan et al. | 2024 | 12 | 2 | HIGH | The Universality Hypothesis in large language models (LLMs) claims that different models converge towards similar concept representations in their latent spaces. Providing evidence for this hypothesis would enable researchers to exploit universal properties, facilitating the generalization of mechanistic interpretability techniques across models. | [Link](https://www.semanticscholar.org/paper/39bbd489b43911152bdaf07f741a91bf1b15989d) |
| 29 | Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups | Davide Ghilardi et al. | 2024 | 10 | 3 | HIGH | SAEs have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained for each model layer. | [Link](https://api.semanticscholar.org/CorpusId:273662212) |
| 30 | Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words | Gouki Minegishi et al. | 2025 | 9 | 3 | HIGH | Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of polysemantic neurons into monosemantic features and composing a sparse dictionary of words. However, traditional performance metrics like Mean Squared Error and L0 sparsity ignore the evaluation of the semantic repr... | [Link](https://www.semanticscholar.org/paper/7471d03a0a72a56b57326792763d9eb9539831d9) |
| 31 | The Origins of Representation Manifolds in Large Language Models | Alexander Modell et al. | 2025 | 8 | 3 | HIGH | There is a large ongoing scientific effort in mechanistic interpretability to map embeddings and internal representations of AI systems into human-understandable concepts. A key element of this effort is the linear representation hypothesis, which posits that neural representations are sparse linear combinations of `almost-orthogonal' direction vectors, reflecting the presence or absence of dif... | [Link](https://api.semanticscholar.org/CorpusId:278905857) |
| 32 | Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders | Kristian Kuznetsov et al. | 2025 | 7 | 3 | HIGH | Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. | [Link](https://api.semanticscholar.org/CorpusId:276781864) |
| 33 | Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders | Charles O'Neill et al. | 2024 | 6 | 3 | HIGH | A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. | [Link](https://api.semanticscholar.org/CorpusId:274149821) |
| 34 | How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models | Mark Ormerod et al. | 2023 | 6 | 3 | HIGH | Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of En... | [Link](https://api.semanticscholar.org/CorpusId:265230959) |
| 35 | Dense SAE Latents Are Features, Not Bugs | Xiaoqing Sun et al. | 2025 | 6 | 3 | HIGH | Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. | [Link](https://api.semanticscholar.org/CorpusId:279448027) |
| 36 | Do Sparse Autoencoders Generalize? A Case Study of Answerability | Lovis Heindrich et al. | 2025 | 6 | 3 | HIGH | Sparse autoencoders (SAEs) have emerged as a promising approach in language model interpretability, offering unsupervised extraction of sparse features. For interpretability methods to succeed, they must identify abstract features across domains, and these features can often manifest differently in each context. | [Link](https://www.semanticscholar.org/paper/fb66116071367a0ebbc747c418dbb0d93d07dfc9) |
| 37 | I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data? | Yuhang Liu et al. | 2025 | 6 | 2 | HIGH | The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. | [Link](https://api.semanticscholar.org/CorpusId:276937847) |
| 38 | PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders | Ahmed Frikha et al. | 2025 | 5 | 3 | HIGH | Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing but also pose significant privacy risks by memorizing and leaking Personally Identifiable Information (PII). Existing mitigation strategies, such as differential privacy and neuron-level interventions, often degrade model utility or fail to effectively prevent leakage. | [Link](https://api.semanticscholar.org/CorpusId:277043418) |
| 39 | Can sparse autoencoders make sense of gene expression latent variable models? | Viktoria Schuster | 2024 | 5 | 3 | HIGH | Sparse autoencoders (SAEs) have lately been used to uncover interpretable latent features in large language models. By projecting dense embeddings into a much higher-dimensional and sparse space, learned features become disentangled and easier to interpret. | [Link](https://api.semanticscholar.org/CorpusId:273351260) |
| 40 | Interpreting token compositionality in LLMs: A robustness analysis | Nura Aljaafari et al. | 2024 | 4 | 3 | HIGH | Understanding the internal mechanisms of large language models (LLMs) is integral to enhancing their reliability, interpretability, and inference processes. We present Constituent-Aware Pooling (CAP), a methodology designed to analyse how LLMs process compositional linguistic structures. | [Link](https://api.semanticscholar.org/CorpusId:273404240) |
| 41 | The Geometry of Harmfulness in LLMs through Subconcept Probing | McNair Shah et al. | 2025 | 3 | 3 | HIGH | Recent advances in large language models (LLMs) have intensified the need to understand and reliably curb their harmful behaviours. We introduce a multidimensional framework for probing and steering harmful content in model internals. | [Link](https://www.semanticscholar.org/paper/c50b632eb52b2c16e1952903da37febed147f29b) |
| 42 | Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality | Sewoong Lee et al. | 2025 | 2 | 3 | HIGH | Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\ell_0$. In this paper, we reveal a theoretical link that the $\ell_2$-norm of t... | [Link](https://api.semanticscholar.org/CorpusId:277468308) |
| 43 | Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification | Xuansheng Wu et al. | 2025 | 2 | 3 | HIGH | Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. | [Link](https://api.semanticscholar.org/CorpusId:276482412) |
| 44 | SAFER: Probing Safety in Reward Models with Sparse Autoencoder | Wei Shi et al. | 2025 | 2 | 3 | HIGH | Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present Sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. | [Link](https://www.semanticscholar.org/paper/38a858d491358347fabdbdeea7d517116c3fa9b9) |
| 45 | LLM Interpretability with Identifiable Temporal-Instantaneous Representation | Xiangchen Song et al. | 2025 | 1 | 3 | HIGH | Despite Large Language Models'remarkable capabilities, understanding their internal representations remains challenging. Mechanistic interpretability tools such as sparse autoencoders (SAEs) were developed to extract interpretable features from LLMs but lack temporal dependency modeling, instantaneous relation representation, and more importantly theoretical guarantees, undermining both the the... | [Link](https://api.semanticscholar.org/CorpusId:281674132) |
| 46 | Teach Old SAEs New Domain Tricks with Boosting | Nikita Koriagin et al. | 2025 | 1 | 3 | HIGH | Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. | [Link](https://api.semanticscholar.org/CorpusId:280232496) |
| 47 | Training Superior Sparse Autoencoders for Instruct Models | Jiaming Li et al. | 2025 | 1 | 3 | HIGH | As large language models (LLMs) grow in scale and capability, understanding their internal mechanisms becomes increasingly critical. Sparse autoencoders (SAEs) have emerged as a key tool in mechanistic interpretability, enabling the extraction of human-interpretable features from LLMs. | [Link](https://api.semanticscholar.org/CorpusId:279250630) |
| 48 | Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability | Leonard Bereska et al. | 2025 | 1 | 3 | HIGH | Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. | [Link](https://www.semanticscholar.org/paper/fa3652f1069bf450dc5667bed6f4e6d4fb97f559) |
| 49 | Signal in the Noise: Polysemantic Interference Transfers and Predicts Cross-Model Influence | Bofan Gong et al. | 2025 | 1 | 3 | HIGH | Polysemanticity is pervasive in language models and remains a major challenge for interpretation and model behavioral control. Leveraging sparse autoencoders (SAEs), we map the polysemantic topology of two small models (Pythia-70M and GPT-2-Small) to identify SAE feature pairs that are semantically unrelated yet exhibit interference within models. | [Link](https://api.semanticscholar.org/CorpusId:278739540) |
| 50 | Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders | Siyu Chen et al. | 2025 | 1 | 3 | HIGH | We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. | [Link](https://www.semanticscholar.org/paper/cf5002b0066926cb62e21f32d547ecc94b9790d7) |
| 51 | Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control | Tsogt-Ochir Enkhbayar | 2025 | 1 | 3 | HIGH | Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (i... | [Link](https://api.semanticscholar.org/CorpusId:283072978) |
| 52 | Geospatial Mechanistic Interpretability of Large Language Models | S. D. Sabbata et al. | 2025 | 1 | 3 | HIGH | Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and"reasoning"tools remains an area of ongoing research. | [Link](https://api.semanticscholar.org/CorpusId:278339325) |
| 53 | Binary Sparse Coding for Interpretability | Lucia Quirke et al. | 2025 | 1 | 3 | HIGH | Sparse autoencoders (SAEs) are used to decompose neural network activations into sparsely activating features, but many SAE features are only interpretable at high activation strengths. To address this issue we propose to use binary sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all activations to be zero or one. | [Link](https://api.semanticscholar.org/CorpusId:281681274) |
| 54 | Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling | Hovhannes Tamoyan et al. | 2025 | 1 | 3 | HIGH | Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). | [Link](https://www.semanticscholar.org/paper/5778bd480af49144fb0a8bc177e13409e95dac47) |
| 55 | Transferring Linear Features Across Language Models With Model Stitching | Alan Chen et al. | 2025 | 1 | 3 | HIGH | In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. | [Link](https://api.semanticscholar.org/CorpusId:279251259) |
| 56 | ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders | Xiangyu Liu et al. | 2025 | 1 | 3 | HIGH | Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic interpretability of large language models. Recent works apply SAE to protein language models (PLMs), aiming to extract and analyze biologically meaningful features from their latent spaces. | [Link](https://www.semanticscholar.org/paper/a1dc2a0ee413b2e0b8b14d47ba41009f40b19e5b) |
| 57 | Mechanistic Interpretability for Neural TSP Solvers | Reuben Narad et al. | 2025 | 1 | 2 | HIGH | Neural networks have advanced combinatorial optimization, with Transformer-based solvers achieving near-optimal solutions on the Traveling Salesman Problem (TSP) in milliseconds. However, these models operate as black boxes, providing no insight into the geometric patterns they learn or the heuristics they employ during tour construction. | [Link](https://api.semanticscholar.org/CorpusId:282384660) |
| 58 | Syntax-guided Neural Module Distillation to Probe Compositionality in Sentence Embeddings | Rohan Pandey | 2023 | 1 | 2 | HIGH | Past work probing compositionality in sentence embedding models faces issues determining the causal impact of implicit syntax representations. Given a sentence, we construct a neural module net based on its syntax parse and train it end-to-end to approximate the sentence’s embedding generated by a transformer model. | [Link](https://api.semanticscholar.org/CorpusId:256105273) |
| 59 | Toward a Flexible Framework for Linear Representation Hypothesis Using Maximum Likelihood Estimation | Trung Nguyen et al. | 2025 | 0 | 3 | HIGH | Linear representation hypothesis posits that high-level concepts are encoded as linear directions in the representation spaces of LLMs. Park et al. | [Link](https://api.semanticscholar.org/CorpusId:276575160) |
| 60 | AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations | Yifei Yao et al. | 2025 | 0 | 3 | HIGH | Understanding the internal representations of large language models (LLMs) remains a central challenge for interpretability research. Sparse autoencoders (SAEs) offer a promising solution by decomposing activations into interpretable features, but existing approaches rely on fixed sparsity constraints that fail to account for input complexity. | [Link](https://api.semanticscholar.org/CorpusId:280710714) |
| 61 | Sparsification and Reconstruction from the Perspective of Representation Geometry | Wenjie Sun et al. | 2025 | 0 | 3 | HIGH | Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic interpretability, aiming to identify interpretable monosemantic features. However, how does sparse encoding organize the representations of activation vector from language models? | [Link](https://api.semanticscholar.org/CorpusId:278959774) |
| 62 | Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations | Omar Claflin | 2025 | 0 | 3 | HIGH | Current sparse autoencoder (SAE) approaches to neural network interpretability assume that activations can be decomposed through linear superposition into sparse, interpretable features. Despite high reconstruction fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit pathological behavioral errors. | [Link](https://www.semanticscholar.org/paper/5b9818aea1e46fa4974f2b5fb9194096cc269793) |
| 63 | OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features | Anton Korznikov et al. | 2025 | 0 | 3 | HIGH | Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural network activations into human-interpretable features. However, current SAEs suffer from feature absorption, where specialized features capture instances of general features creating representation holes, and feature composition, where independent features merge into composite representations. | [Link](https://api.semanticscholar.org/CorpusId:281658756) |
| 64 | FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies | Seonglae Cho et al. | 2025 | 0 | 3 | HIGH | Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. | [Link](https://api.semanticscholar.org/CorpusId:279999731) |
| 65 | SplInterp: Improving our Understanding and Training of Sparse Autoencoders | Jeremy Budd et al. | 2025 | 0 | 3 | HIGH | Sparse autoencoders (SAEs) have received considerable recent attention as tools for mechanistic interpretability, showing success at extracting interpretable features even from very large LLMs. However, this research has been largely empirical, and there have been recent doubts about the true utility of SAEs. | [Link](https://api.semanticscholar.org/CorpusId:278740696) |
| 66 | Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders | Jaron Cohen et al. | 2025 | 0 | 3 | HIGH | Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. | [Link](https://api.semanticscholar.org/CorpusId:283711466) |
| 67 | Mechanistic Interpretability of Antibody Language Models Using SAEs | Rebonto Haque et al. | 2025 | 0 | 3 | HIGH | Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. | [Link](https://api.semanticscholar.org/CorpusId:283671976) |
| 68 | Superposition disentanglement of neural representations reveals hidden alignment | André Longon et al. | 2025 | 0 | 3 | HIGH | The superposition hypothesis states that single neurons may participate in representing multiple features in order for the neural network to represent more features than it has neurons. In neuroscience and AI, representational alignment metrics measure the extent to which different deep neural networks (DNNs) or brains represent similar information. | [Link](https://www.semanticscholar.org/paper/cb38251051504d489d64232ba11f5b8dae547424) |
| 69 | Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations | Da Ma et al. | 2025 | 0 | 3 | HIGH | Instruction tuning improves the ability of large language models (LLMs) to follow diverse human instructions, but achieving strong performance on specific target tasks remains challenging. A critical bottleneck is selecting the most relevant data to maximize task-specific performance. | [Link](https://api.semanticscholar.org/CorpusId:277150920) |
| 70 | Semantic Convergence: Investigating Shared Representations Across Scaled LLMs | Daniel Son et al. | 2025 | 0 | 3 | HIGH | We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts. Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, an... | [Link](https://api.semanticscholar.org/CorpusId:280400969) |
| 71 | The Logical Implication Steering Method for Conditional Interventions on Transformer Generation | Damjan Kalajdzievski | 2025 | 0 | 3 | HIGH | The field of mechanistic interpretability in pre-trained transformer models has demonstrated substantial evidence supporting the''linear representation hypothesis'', which is the idea that high level concepts are encoded as vectors in the space of activations of a model. Studies also show that model generation behavior can be steered toward a given concept by adding the concept's vector to the ... | [Link](https://www.semanticscholar.org/paper/5500d7948c7ab2e4d098c608748a272f2328484d) |
| 72 | Linear socio-demographic representations emerge in Large Language Models from indirect cues | Paul Bouchaud et al. | 2025 | 0 | 3 | HIGH | We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. | [Link](https://www.semanticscholar.org/paper/2d850838ba054945d7b65588ee016b2fd51aa4ee) |
| 73 | Probing BERT for German Compound Semantics | Filip Mileti'c et al. | 2025 | 0 | 3 | HIGH | This paper investigates the extent to which pretrained German BERT encodes knowledge of noun compound semantics. We comprehensively vary combinations of target tokens, layers, and cased vs. | [Link](https://api.semanticscholar.org/CorpusId:278769263) |
| 74 | CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders | Alex Gulko et al. | 2025 | 0 | 3 | HIGH | Sparse autoencoders (SAEs) are a promising approach for uncovering interpretable features in large language models (LLMs). While several automated evaluation methods exist for SAEs, most rely on external LLMs. | [Link](https://www.semanticscholar.org/paper/70be79af59c8cb3e2c8c4f4f78cbb9a9fe640e1d) |
| 75 | Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability | Usha Bhalla et al. | 2025 | 0 | 3 | HIGH | Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual informa... | [Link](https://www.semanticscholar.org/paper/fa965a002729ce91605d9b759724409b95ce983d) |
| 76 | SAE-RNA: A Sparse Autoencoder Model for Interpreting RNA Language Model Representations | Taehan Kim et al. | 2025 | 0 | 3 | HIGH | Deep learning, particularly with the advancement of Large Language Models, has transformed biomolecular modeling, with protein advances (e.g., ESM) inspiring emerging RNA language models such as RiNALMo. Yet how and what these RNA Language Models internally encode about messenger RNA (mRNA) or non-coding RNA (ncRNA) families remains unclear. | [Link](https://www.semanticscholar.org/paper/874f2b6c464def4848eb1c3f9dbb937f94055482) |
| 77 | Analysis of Variational Sparse Autoencoders | Zachary Baker et al. | 2025 | 0 | 3 | HIGH | Sparse Autoencoders (SAEs) have emerged as a promising approach for interpreting neural network representations by learning sparse, human-interpretable features from dense activations. We investigate whether incorporating variational methods into SAE architectures can improve feature organization and interpretability. | [Link](https://api.semanticscholar.org/CorpusId:281676369) |
| 78 | Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training | T. E. Li et al. | 2025 | 0 | 3 | HIGH | Understanding the internal representations of large language models is crucial for ensuring their reliability and safety, with sparse autoencoders (SAEs) emerging as a promising interpretability approach. However, current SAE training methods face feature absorption, where features (or neurons) are absorbed into each other to minimize $L_1$ penalty, making it difficult to consistently identify ... | [Link](https://www.semanticscholar.org/paper/c89bf8b8c0b1f1bfc1dbb44c73f4c9d429ba8e83) |
| 79 | Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy | Nikita Balagansky et al. | 2025 | 0 | 3 | HIGH | Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computation... | [Link](https://www.semanticscholar.org/paper/d3779f55663c96087c916c89e1065f908cb41afe) |
| 80 | Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features | Yiting Liu et al. | 2026 | 0 | 3 | HIGH | Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. | [Link](https://www.semanticscholar.org/paper/8522abd2e8f7487bb88881be5b00dd8570b5cdfa) |
| 81 | Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders | Ruikang Zhang et al. | 2026 | 0 | 3 | HIGH | Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. | [Link](https://www.semanticscholar.org/paper/5319fc4c32e89759f67caa6c4653d0c061e73926) |
| 82 | Evaluating Sparse Autoencoders for Monosemantic Representation | Moghis Fereidouni et al. | 2025 | 0 | 2 | HIGH | A key barrier to interpreting large language models is polysemanticity, where neurons activate for multiple unrelated concepts. Sparse autoencoders (SAEs) have been proposed to mitigate this issue by transforming dense activations into sparse, more interpretable features. | [Link](https://www.semanticscholar.org/paper/01db059c28246277f636b0e283a20d73f04750f9) |
| 83 | Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit | Val'erie Costa et al. | 2025 | 0 | 2 | HIGH | Sparse autoencoders (SAEs) have recently become central tools for interpretability, leveraging dictionary learning principles to extract sparse, interpretable features from neural representations whose underlying structure is typically unknown. This paper evaluates SAEs in a controlled setting using MNIST, which reveals that current shallow architectures implicitly rely on a quasi-orthogonality... | [Link](https://api.semanticscholar.org/CorpusId:279244299) |
| 84 | GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models | Mariam Mahran et al. | 2025 | 0 | 2 | HIGH | Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. | [Link](https://api.semanticscholar.org/CorpusId:281725135) |
| 85 | Superposition in Transformers: A Novel Way of Building Mixture of Experts | Ayoub Ben Chaliah et al. | 2024 | 0 | 2 | HIGH | Catastrophic forgetting remains a major challenge when adapting large language models (LLMs) to new tasks or domains. Conventional fine-tuning often overwrites existing knowledge, causing performance degradation on original tasks. | [Link](https://api.semanticscholar.org/CorpusId:275212824) |
| 86 | Decomposing multimodal embedding spaces with group-sparse autoencoders | Chiraag Kaushik et al. | 2026 | 0 | 2 | HIGH | The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspon... | [Link](https://www.semanticscholar.org/paper/78373e65a13837de405c49f0671a6707278f90d5) |
| 87 | Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders | K. Tahimic et al. | 2025 | 0 | 2 | HIGH | As Large Language Models become integral to software development, with substantial portions of AI-suggested code entering production, understanding their internal correctness mechanisms becomes critical for safe deployment. We apply sparse autoencoders to decompose LLM representations, identifying directions that correspond to code correctness. | [Link](https://www.semanticscholar.org/paper/293139515d9b85edb92b9e7cdcefdf63652bad54) |
| 88 | Atlas-Alignment: Making Interpretability Transferable Across Language Models | Bruno Puri et al. | 2025 | 0 | 2 | HIGH | Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. | [Link](https://www.semanticscholar.org/paper/41e936b755c5a3ddfbc53904edd0d9b0f1bc9381) |
| 89 | Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space | Mor Geva et al. | 2022 | 475 | 2 | MEDIUM | Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. | [Link](https://www.semanticscholar.org/paper/cf36236015c9f93f15bfafbf282f69e08bdc9c16) |
| 90 | The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings | Austin C. Kozlowski et al. | 2018 | 445 | 2 | MEDIUM | We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. | [Link](https://www.semanticscholar.org/paper/83df30bae85fd5888665387310a04abc35207568) |
| 91 | Emergent linguistic structure in artificial neural networks trained by self-supervision | Christopher D. Manning et al. | 2020 | 370 | 2 | MEDIUM | This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. | [Link](https://www.semanticscholar.org/paper/04ef54bd467d5e03dee7b0be601cf06d420bffa0) |
| 92 | Steering Language Models With Activation Engineering | Alexander Matt Turner et al. | 2023 | 327 | 2 | MEDIUM | Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. | [Link](https://www.semanticscholar.org/paper/fe303bbaae47b1b08d0641b41d3288fcd74a3a80) |
| 93 | Mechanistic Interpretability for AI Safety - A Review | Leonard Bereska et al. | 2024 | 317 | 3 | MEDIUM | Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. | [Link](https://api.semanticscholar.org/CorpusId:269293418) |
| 94 | Linear Algebraic Structure of Word Senses, with Applications to Polysemy | Sanjeev Arora et al. | 2016 | 316 | 2 | MEDIUM | Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. | [Link](https://www.semanticscholar.org/paper/45ea9f9ae5368774d921e56e28fade358d171b2f) |
| 95 | Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn’t. | Anna Rogers et al. | 2016 | 264 | 2 | MEDIUM | Following up on numerous reports of analogy-based identiﬁcation of “linguistic regularities” in word embeddings, this study applies the widely used vector offset method to 4 types of linguistic relations: inﬂectional and derivational morphology, and lexicographic and en-cyclopedic semantics. We present a balanced test set with 99,200 questions in 40 categories, and we systematically examine how... | [Link](https://www.semanticscholar.org/paper/3f31c4e394d5f5623b95b832d182996c622c75cd) |
| 96 | Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models | Samuel Marks et al. | 2024 | 260 | 3 | MEDIUM | We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. | [Link](https://api.semanticscholar.org/CorpusId:268732732) |
| 97 | Language Models Represent Space and Time | Wes Gurnee et al. | 2023 | 244 | 2 | MEDIUM | The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (histor... | [Link](https://www.semanticscholar.org/paper/740c783ac07039cf30b6d8a8f95e775b3297c79e) |
| 98 | Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen | Aleksandr Drozd et al. | 2016 | 168 | 2 | MEDIUM |  | [Link](https://www.semanticscholar.org/paper/686b52953471a9d7a515215ba54ad0350c6b0472) |
| 99 | Linearity of Relation Decoding in Transformer Language Models | Evan Hernandez et al. | 2023 | 142 | 2 | MEDIUM | Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. | [Link](https://www.semanticscholar.org/paper/55c562c0de9d011c91965a34ba784c9d4b72fecb) |
| 100 | A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models | Daking Rai et al. | 2024 | 88 | 3 | MEDIUM | Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. | [Link](https://www.semanticscholar.org/paper/2ac231b9cff4f5f9054d86c9b540429d4dd687f4) |
| 101 | Language Models Implement Simple Word2Vec-style Vector Arithmetic | Jack Merullo et al. | 2023 | 86 | 3 | MEDIUM | A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). | [Link](https://www.semanticscholar.org/paper/95a0c8feccc01f2799c961ca850f75f9b054de6c) |
| 102 | The Geometry of Multilingual Language Model Representations | Tyler A. Chang et al. | 2022 | 86 | 2 | MEDIUM | We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. | [Link](https://www.semanticscholar.org/paper/24a4657b614a4a3037bf045cc1ded0548771c148) |
| 103 | Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization | Boshi Wang et al. | 2024 | 76 | 3 | MEDIUM | We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. | [Link](https://api.semanticscholar.org/CorpusId:270045579) |
| 104 | Probing for idiomaticity in vector space models | Marcos Garcia et al. | 2021 | 61 | 3 | MEDIUM | Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sens... | [Link](https://api.semanticscholar.org/CorpusId:233189591) |
| 105 | RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations | Jing Huang et al. | 2024 | 58 | 3 | MEDIUM | Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? | [Link](https://api.semanticscholar.org/CorpusId:268032000) |
| 106 | SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability | Adam Karvonen et al. | 2025 | 56 | 3 | MEDIUM | Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. | [Link](https://api.semanticscholar.org/CorpusId:276937927) |
| 107 | Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning | Dan Braun et al. | 2024 | 56 | 3 | MEDIUM | Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. | [Link](https://api.semanticscholar.org/CorpusId:269929791) |
| 108 | Polysemanticity and Capacity in Neural Networks | Adam Scherlis et al. | 2022 | 51 | 2 | MEDIUM | Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. | [Link](https://www.semanticscholar.org/paper/02cf0e7a782b6c0f04863f9c9ee5b4e77394deb6) |
| 109 | Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation | Verna Dankers et al. | 2022 | 43 | 3 | MEDIUM | Unlike literal expressions, idioms’ meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. | [Link](https://www.semanticscholar.org/paper/ce975a743822a1bc89ef6bf182388f41866225b5) |
| 110 | Sparse autoencoders reveal selective remapping of visual concepts during adaptation | Hyesu Lim et al. | 2024 | 30 | 3 | MEDIUM | Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. | [Link](https://www.semanticscholar.org/paper/3b748d1f2b314f8bb2ab4123946bd58c607738f3) |
| 111 | Towards Uncovering How Large Language Model Works: An Explainability Perspective | Haiyan Zhao et al. | 2024 | 25 | 3 | MEDIUM | Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. | [Link](https://api.semanticscholar.org/CorpusId:267740636) |
| 112 | Interpretable Steering of Large Language Models with Feature Guided Activation Additions | Samuel Soo et al. | 2025 | 23 | 3 | MEDIUM | Effective and reliable control over large language model (LLM) behavior is a significant challenge. While activation steering methods, which add steering vectors to a model's hidden states, are a promising approach, existing techniques often lack precision and interpretability in how they influence model outputs. | [Link](https://www.semanticscholar.org/paper/89f55a1eb9bb1794998cb0cd4251abf8c1964a97) |
| 113 | Do LLMs "know" internally when they follow instructions? | Juyeon Heo et al. | 2024 | 22 | 2 | MEDIUM | Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. | [Link](https://www.semanticscholar.org/paper/133e0fcb3a3a903e765844ae98d367795b8f3d8d) |
| 114 | Automatically Identifying Local and Global Circuits with Linear Computation Graphs | Xuyang Ge et al. | 2024 | 20 | 3 | MEDIUM | Circuit analysis of any certain model behavior is a central task in mechanistic interpretability. We introduce our circuit discovery pipeline with Sparse Autoencoders (SAEs) and a variant called Transcoders. | [Link](https://www.semanticscholar.org/paper/9e08a7385a3908ecfaa7886c8597f8c533672ca0) |
| 115 | Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders | Xuansheng Wu et al. | 2025 | 20 | 3 | MEDIUM | Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. | [Link](https://api.semanticscholar.org/CorpusId:276557992) |
| 116 | What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes | Victor Lecomte et al. | 2023 | 19 | 2 | MEDIUM | Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features"than neurons, such that learning to perform a task forces the network to co-allocate multiple unre... | [Link](https://www.semanticscholar.org/paper/cd4cdb622e30e799789c63e8f1b5cedd91fc0506) |
| 117 | Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution | Haiyan Zhao et al. | 2024 | 18 | 3 | MEDIUM | Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. | [Link](https://api.semanticscholar.org/CorpusId:273022717) |
| 118 | Sparse Autoencoder Features for Classifications and Transferability | Jack Gallifant et al. | 2025 | 15 | 3 | MEDIUM | Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. | [Link](https://api.semanticscholar.org/CorpusId:276408514) |
| 119 | Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models | Mateusz Pach et al. | 2025 | 15 | 3 | MEDIUM | Sparse Autoencoders (SAEs) have recently gained attention as a means to improve the interpretability and steerability of Large Language Models (LLMs), both of which are essential for AI safety. In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in visual repres... | [Link](https://api.semanticscholar.org/CorpusId:277510503) |
| 120 | Understanding polysemanticity in neural networks through coding theory | Simon Marshall et al. | 2024 | 15 | 2 | MEDIUM | Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. | [Link](https://www.semanticscholar.org/paper/73a5986b29d7518abde0f65d29f7031b2b911292) |
| 121 | Towards Inference-time Category-wise Safety Steering for Large Language Models | Amrita Bhattacharjee et al. | 2024 | 15 | 2 | MEDIUM | While large language models (LLMs) have seen unprecedented advancements in capabilities and applications across a variety of use-cases, safety alignment of these models is still an area of active research. The fragile nature of LLMs, even models that have undergone extensive alignment and safety training regimes, warrants additional safety steering steps via training-free, inference-time methods. | [Link](https://www.semanticscholar.org/paper/8811ed198ba95dcfc0bef493088218e957eb168d) |
| 122 | Analyzing (In)Abilities of SAEs via Formal Languages | Abhinav Menon et al. | 2024 | 14 | 3 | MEDIUM | Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. | [Link](https://api.semanticscholar.org/CorpusId:273351117) |
| 123 | A Psycholinguistic Analysis of BERT’s Representations of Compounds | Lars Buijtelaar et al. | 2023 | 13 | 3 | MEDIUM | This work studies the semantic representations learned by BERT for compounds, that is, expressions such as sunlight or bodyguard. We build on recent studies that explore semantic information in Transformers at the word level and test whether BERT aligns with human semantic intuitions when dealing with expressions (e.g., sunlight) whose overall meaning depends—to a various extent—on the semantic... | [Link](https://api.semanticscholar.org/CorpusId:256846381) |
| 124 | Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts | Kenny Peng et al. | 2025 | 12 | 3 | MEDIUM | While sparse autoencoders (SAEs) have generated significant excitement, a series of negative results have added to skepticism about their usefulness. Here, we establish a conceptual distinction that reconciles competing narratives surrounding SAEs. | [Link](https://api.semanticscholar.org/CorpusId:280012198) |
| 125 | Beyond the Black Box: Interpretability of LLMs in Finance | Hariom Tatsat et al. | 2025 | 10 | 3 | MEDIUM | Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financia... | [Link](https://api.semanticscholar.org/CorpusId:279071311) |
| 126 | Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models | Aashiq Muhamed et al. | 2024 | 10 | 3 | MEDIUM | Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. | [Link](https://api.semanticscholar.org/CorpusId:273798337) |
| 127 | On the Complexity of Neural Computation in Superposition | Micah Adler et al. | 2024 | 10 | 2 | MEDIUM | Superposition, the ability of neural networks to represent more features than neurons, is increasingly seen as key to the efficiency of large models. This paper investigates the theoretical foundations of computing in superposition, establishing complexity bounds for explicit, provably correct algorithms. | [Link](https://api.semanticscholar.org/CorpusId:272832470) |
| 128 | TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation | Victor Shea-Jay Huang et al. | 2025 | 6 | 3 | MEDIUM | Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion architectures. We propose TIDE-Temporal-aware sparse autoencoders for Interpretable Diffusion transformErs-a framework designed to extract sparse, interpretable activation features across timesteps in DiTs. | [Link](https://www.semanticscholar.org/paper/b306e96667f4b6b7888ec79e4ee1ca4429ef4a6f) |
| 129 | Analyze Feature Flow to Enhance Interpretation and Steering in Language Models | Daniil Laptev et al. | 2025 | 6 | 3 | MEDIUM | We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. | [Link](https://api.semanticscholar.org/CorpusId:276116297) |
| 130 | Model Unlearning via Sparse Autoencoder Subspace Guided Projections | Xu Wang et al. | 2025 | 6 | 3 | MEDIUM | Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. | [Link](https://www.semanticscholar.org/paper/8fb3a639c6b87d29d4be5ccbd4ca6f66b7aa8bc2) |
| 131 | FADE: Why Bad Descriptions Happen to Good Features | Bruno Puri et al. | 2025 | 6 | 3 | MEDIUM | Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While this may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. | [Link](https://api.semanticscholar.org/CorpusId:276574664) |
| 132 | Exploring the LLM Journey from Cognition to Expression with Linear Representations | Yuzi Yan et al. | 2024 | 6 | 2 | MEDIUM | This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pret... | [Link](https://www.semanticscholar.org/paper/776b2b7f140a2407b8a8683b2615c5f2a00f6d4c) |
| 133 | Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets | Duanyu Feng et al. | 2024 | 5 | 3 | MEDIUM | The success of the reward model in distinguishing between responses with subtle safety differences depends critically on the high-quality preference dataset, which should capture the fine-grained nuances of harmful and harmless responses. This motivates the need to develop the datasets involving preference margins, which accurately quantify how harmless one response is compared to another. | [Link](https://www.semanticscholar.org/paper/ec3529fe265b84944ac3f63686edadbcede8972c) |
| 134 | Interpreting Learned Feedback Patterns in Large Language Models | Luke Marks et al. | 2023 | 5 | 2 | MEDIUM | Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. | [Link](https://www.semanticscholar.org/paper/05c119cab0934eac0ce8d2b3884f270b66151ac9) |
| 135 | On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond | Jingyi Cui et al. | 2025 | 4 | 3 | MEDIUM | Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting features learned by large language models (LLMs). It aims to recover complex superposed polysemantic features into interpretable monosemantic ones through feature reconstruction via sparsely activated neural networks. | [Link](https://api.semanticscholar.org/CorpusId:279464643) |
| 136 | Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness | Qi Zhang et al. | 2024 | 4 | 3 | MEDIUM | Deep learning models often suffer from a lack of interpretability due to polysemanticity, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in monosemanticity, where neurons correspond to consistent and distinct semantics, have significantly improved interpretability but are commonly believed to compromis... | [Link](https://www.semanticscholar.org/paper/bd0af191ffc6f06be0a8cbe42a28b88c703de1e3) |
| 137 | Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders | David Chanin et al. | 2025 | 4 | 3 | MEDIUM | It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more narrow than the number of underlying"true features"on which it is trained, and there is correlation between features, the SAE will merge compone... | [Link](https://www.semanticscholar.org/paper/ea935701fc581b6018308d06eea6f17b88a63432) |
| 138 | Steering Large Language Models for Vulnerability Detection | Jiayuan Li et al. | 2025 | 4 | 3 | MEDIUM | Vulnerability detection remains a critical challenge in the field of security. Many existing approaches extract code representations for vulnerability detection. | [Link](https://www.semanticscholar.org/paper/abf766afb883b349f938413ac8fd18b92a13defb) |
| 139 | Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems | Simon Lermen et al. | 2025 | 4 | 3 | MEDIUM | We demonstrate how AI agents can coordinate to deceive oversight systems using automated interpretability of neural networks. Using sparse autoencoders (SAEs) as our experimental framework, we show that language models (Llama, DeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that evade detection. | [Link](https://api.semanticscholar.org/CorpusId:277667390) |
| 140 | Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning | Jeffrey Olmo et al. | 2024 | 4 | 3 | MEDIUM | Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network's internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. | [Link](https://api.semanticscholar.org/CorpusId:274116871) |
| 141 | A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations | Charles O'Neill et al. | 2025 | 4 | 2 | MEDIUM | Contextual hallucinations -- statements unsupported by given context -- remain a significant challenge in AI. We demonstrate a practical interpretability insight: a generator-agnostic observer model detects hallucinations via a single forward pass and a linear probe on its residual stream. | [Link](https://www.semanticscholar.org/paper/576531260abc89ed3a08a6ce552706e7860e9394) |
| 142 | Evaluating Synthetic Activations composed of SAE Latents in GPT-2 | Giorgi Giglemiani et al. | 2024 | 3 | 3 | MEDIUM | Sparse Auto-Encoders (SAEs) are commonly employed in mechanistic interpretability to decompose the residual stream into monosemantic SAE latents. Recent work demonstrates that perturbing a model's activations at an early layer results in a step-function-like change in the model's final layer activations. | [Link](https://api.semanticscholar.org/CorpusId:272827041) |
| 143 | Priors in Time: Missing Inductive Biases for Language Model Interpretability | E. Lubana et al. | 2025 | 3 | 3 | MEDIUM | Recovering meaningful concepts from language model activations is a central aim of interpretability. While existing feature extraction methods aim to identify concepts that are independent directions, it is unclear if this assumption can capture the rich temporal structure of language. | [Link](https://www.semanticscholar.org/paper/5131a77b3f58ecce7a04b6683c8d7874dbaaf9dd) |
| 144 | Emergence of Linear Truth Encodings in Language Models | Shauli Ravfogel et al. | 2025 | 3 | 2 | MEDIUM | Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce a transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. | [Link](https://www.semanticscholar.org/paper/b8061826f8b565a6f8b55dbe6faa9ab574f49106) |
| 145 | On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions | Dang Nguyen et al. | 2025 | 3 | 2 | MEDIUM | Understanding and mitigating biases is critical for the adoption of large language models (LLMs) in high-stakes decision-making. We introduce Admissions and Hiring, decision tasks with hypothetical applicant profiles where a person's race can be inferred from their name, as simplified test beds for racial bias. | [Link](https://www.semanticscholar.org/paper/7ae78424959610c26413035dffa164c8f09ca06a) |
| 146 | Semantic Structure in Large Language Model Embeddings | Austin C. Kozlowski et al. | 2025 | 2 | 3 | MEDIUM | Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. | [Link](https://www.semanticscholar.org/paper/eba2b9b4be9d9cc3cb74a162c2a9de88cbbaf189) |
| 147 | Accelerating Sparse Autoencoder Training via Layer-Wise Transfer Learning in Large Language Models | Davide Ghilardi et al. | 2024 | 2 | 3 | MEDIUM | Sparse AutoEncoders (SAEs) have gained popularity as a tool for enhancing the interpretability of Large Language Models (LLMs). However, training SAEs can be computationally intensive, especially as model complexity grows. | [Link](https://www.semanticscholar.org/paper/cb8c55997787c28e9eb733bd9e6ce605973e5685) |
| 148 | Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models | Femi Bello et al. | 2025 | 2 | 2 | MEDIUM | It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. | [Link](https://www.semanticscholar.org/paper/afc17f1f339b0c93f41ceb05850cb15f168ca2b8) |
| 149 | Mechanistic interpretability for steering vision-language-action models | Bear Häon et al. | 2025 | 2 | 2 | MEDIUM | Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. | [Link](https://www.semanticscholar.org/paper/c63324aadee7bc9566a98fef80e2149d21a3360a) |
| 150 | Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks | Bianka Kowalska et al. | 2025 | 1 | 3 | MEDIUM | The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. | [Link](https://api.semanticscholar.org/CorpusId:283244330) |
| 151 | Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval | Seongwan Park et al. | 2025 | 1 | 3 | MEDIUM | Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability. In this work, we propose a novel interpretability framework that leverages Sparse Autoencoders (SAEs) to decompose previously uninterpretable dense embeddings from DPR models into distinct, interpretable latent concepts. | [Link](https://api.semanticscholar.org/CorpusId:279075388) |
| 152 | Revisiting End-To-End Sparse Autoencoder Training: A Short Finetune Is All You Need | Adam Karvonen | 2025 | 1 | 3 | MEDIUM | Sparse autoencoders (SAEs) are widely used for interpreting language model activations. A key evaluation metric is the increase in cross-entropy loss between the original model logits and the reconstructed model logits when replacing model activations with SAE reconstructions. | [Link](https://www.semanticscholar.org/paper/f0ca3bb7de01105dae70f13835718914df7b72f1) |
| 153 | Unveiling the Latent Directions of Reflection in Large Language Models | Fu-Chieh Chang et al. | 2025 | 1 | 2 | MEDIUM | Reflection, the ability of large language models (LLMs) to evaluate and revise their own reasoning, has been widely used to improve performance on complex reasoning tasks. Yet, most prior works emphasizes designing reflective prompting strategies or reinforcement learning objectives, leaving the inner mechanisms of reflection underexplored. | [Link](https://api.semanticscholar.org/CorpusId:280711509) |
| 154 | SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs | Zhenliang Zhang et al. | 2025 | 1 | 2 | MEDIUM | Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. | [Link](https://api.semanticscholar.org/CorpusId:282911451) |
| 155 | Superposition in Graph Neural Networks | Lukas Pertl et al. | 2025 | 1 | 2 | MEDIUM | Interpreting graph neural networks (GNNs) is difficult because message passing mixes signals and internal channels rarely align with human concepts. We study superposition, the sharing of directions by multiple features, directly in the latent space of GNNs. | [Link](https://api.semanticscholar.org/CorpusId:281079906) |
| 156 | A Financial Brain Scan of the LLM | Hui Chen et al. | 2025 | 0 | 3 | MEDIUM | Emerging techniques in computer science make it possible to"brain scan"large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without re... | [Link](https://api.semanticscholar.org/CorpusId:280984845) |
| 157 | Computational Basis of LLM's Decision Making in Social Simulation | Ji Ma | 2025 | 0 | 3 | MEDIUM | Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. | [Link](https://api.semanticscholar.org/CorpusId:277824488) |
| 158 | Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder | Zhen Xu et al. | 2025 | 0 | 3 | MEDIUM | Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs'hidden layers have high dimensionality to satisfy ... | [Link](https://api.semanticscholar.org/CorpusId:282911327) |
| 159 | Linear representations in language models can change dramatically over a conversation | Andrew Kyle Lampinen et al. | 2026 | 0 | 3 | MEDIUM | Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. | [Link](https://www.semanticscholar.org/paper/7fde875cf169146a275daef1ff5575ffdcd17947) |
| 160 | The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models | Zhixiang Wang | 2025 | 0 | 3 | MEDIUM | Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an"alignment tax"-- degrading general reasoning capabilities. | [Link](https://www.semanticscholar.org/paper/f2d219ed193875d068f1d5e3eb45ef9e8f0c9a03) |
| 161 | Detecting and Steering LLMs' Empathy in Action | Juan P. Cadile | 2025 | 0 | 3 | MEDIUM | We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored). | [Link](https://www.semanticscholar.org/paper/9d8ca87e7378a4e8ca9cf0fd1fb2de7805f4d768) |
| 162 | Evaluating SAE interpretability without explanations | Gonccalo Paulo et al. | 2025 | 0 | 3 | MEDIUM | Sparse autoencoders (SAEs) and transcoders have become important tools for machine learning interpretability. However, measuring how interpretable they are remains challenging, with weak consensus about which benchmarks to use. | [Link](https://www.semanticscholar.org/paper/b0acc8ded9db6424ae27c14834b3846386028a49) |
| 163 | Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation | Daniel Zhao et al. | 2025 | 0 | 3 | MEDIUM | We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices repres... | [Link](https://www.semanticscholar.org/paper/31f5b1e34c83329dd618768cf9290f10ef626af8) |
| 164 | Binary Autoencoder for Mechanistic Interpretability of Large Language Models | Hakaze Cho et al. | 2025 | 0 | 3 | MEDIUM | Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global s... | [Link](https://api.semanticscholar.org/CorpusId:281526077) |
| 165 | Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework | Jiaqi Weng et al. | 2025 | 0 | 3 | MEDIUM | Increasing deployment of large language models (LLMs) in real-world applications raises significant safety concerns. Most existing safety research focuses on evaluating LLM outputs or specific safety tasks, limiting their ability to address broader, undefined risks. | [Link](https://www.semanticscholar.org/paper/f5318a0b635adfb397bd201dcd0f587104167a12) |
| 166 | Structure before the Machine: Input Space is the Prerequisite for Concepts | Bowei Tian et al. | 2025 | 0 | 2 | MEDIUM | High-level representations have become a central focus in enhancing AI transparency and control, shifting attention from individual neurons or circuits to structured semantic directions that align with human-interpretable concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned directions origina... | [Link](https://www.semanticscholar.org/paper/84c1ad090ceb98f6bf10e62b1c563b4fb151e60c) |
| 167 | Vision Language Model Interpretability with Concept Guided Decoding | Pedro Valois et al. | 2025 | 0 | 2 | MEDIUM |  | [Link](https://www.semanticscholar.org/paper/4578b1bbbd61783e3c7c50e39290aa4fbd158c48) |
| 168 | Confidence is Not Competence | Debdeep Sanyal et al. | 2025 | 0 | 2 | MEDIUM | Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. | [Link](https://www.semanticscholar.org/paper/06294f42f67b63cf03faf7309ebc2eaab5fbfb13) |
| 169 | A Structural Probe for Finding Syntax in Word Representations | John Hewitt et al. | 2019 | 1251 | 2 | LOW | Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. | [Link](https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675) |