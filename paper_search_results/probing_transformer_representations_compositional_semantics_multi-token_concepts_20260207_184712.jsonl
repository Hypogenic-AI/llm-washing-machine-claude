{"title": "How Is a \u201cKitchen Chair\u201d like a \u201cFarm Horse\u201d? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models", "year": 2023, "authors": "Mark Ormerod, Barry Devereux, Jes\u00fas Mart\u00ednez del Rinc\u00f3n", "url": "https://api.semanticscholar.org/CorpusId:265230959", "relevance": 3, "abstract": "Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel \u201ccompositional probe\u201d setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.", "citations": 6}
{"title": "Interpreting token compositionality in LLMs: A robustness analysis", "year": 2024, "authors": "Nura Aljaafari, Danilo S. Carvalho, Andr\u00e9 Freitas", "url": "https://api.semanticscholar.org/CorpusId:273404240", "relevance": 3, "abstract": "Understanding the internal mechanisms of large language models (LLMs) is integral to enhancing their reliability, interpretability, and inference processes. We present Constituent-Aware Pooling (CAP), a methodology designed to analyse how LLMs process compositional linguistic structures. Grounded in principles of compositionality, mechanistic interpretability, and information theory, CAP systematically intervenes in model activations through constituent-based pooling at various model levels. Our experiments on inverse definition modelling, hypernym and synonym prediction reveal critical insights into transformers' limitations in handling compositional abstractions. No specific layer integrates tokens into unified semantic representations based on their constituent parts. We observe fragmented information processing, which intensifies with model size, suggesting that larger models struggle more with these interventions and exhibit greater information dispersion. This fragmentation likely stems from transformers' training objectives and architectural design, preventing systematic and cohesive representations. Our findings highlight fundamental limitations in current transformer architectures regarding compositional semantics processing and model interpretability, underscoring the critical need for novel approaches in LLM design to address these challenges.", "citations": 4}
{"title": "Probing BERT for German Compound Semantics", "year": 2025, "authors": "Filip Mileti'c, Aaron Schmid, Sabine Schulte im Walde", "url": "https://api.semanticscholar.org/CorpusId:278769263", "relevance": 3, "abstract": "This paper investigates the extent to which pretrained German BERT encodes knowledge of noun compound semantics. We comprehensively vary combinations of target tokens, layers, and cased vs. uncased models, and evaluate them by predicting the compositionality of 868 gold standard compounds. Looking at representational patterns within the transformer architecture, we observe trends comparable to equivalent prior work on English, with compositionality information most easily recoverable in the early layers. However, our strongest results clearly lag behind those reported for English, suggesting an inherently more difficult task in German. This may be due to the higher productivity of compounding in German than in English and the associated increase in constituent-level ambiguity, including in our target compound set.", "citations": 0}
{"title": "Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation", "year": 2022, "authors": "Verna Dankers, Christopher Lucas, Ivan Titov", "url": "https://www.semanticscholar.org/paper/ce975a743822a1bc89ef6bf182388f41866225b5", "relevance": 3, "abstract": "Unlike literal expressions, idioms\u2019 meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model, Transformer, by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language.When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expressions. This manifests in idioms\u2019 parts being grouped through attention and in reduced interaction between idioms and their context.In the decoder\u2019s cross-attention, figurative inputs result in reduced attention on source-side tokens. These results suggest that Transformer\u2019s tendency to process idioms as compositional expressions contributes to literal translations of idioms.", "citations": 43}
{"title": "Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization", "year": 2024, "authors": "Boshi Wang, Xiang Yue, Yu Su, Huan Sun", "url": "https://api.semanticscholar.org/CorpusId:270045579", "relevance": 3, "abstract": "We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.", "citations": 76}
{"title": "A Psycholinguistic Analysis of BERT\u2019s Representations of Compounds", "year": 2023, "authors": "Lars Buijtelaar, Sandro Pezzelle", "url": "https://api.semanticscholar.org/CorpusId:256846381", "relevance": 3, "abstract": "This work studies the semantic representations learned by BERT for compounds, that is, expressions such as sunlight or bodyguard. We build on recent studies that explore semantic information in Transformers at the word level and test whether BERT aligns with human semantic intuitions when dealing with expressions (e.g., sunlight) whose overall meaning depends\u2014to a various extent\u2014on the semantics of the constituent words (sun, light). We leverage a dataset that includes human judgments on two psycholinguistic measures of compound semantic analysis: lexeme meaning dominance (LMD; quantifying the weight of each constituent toward the compound meaning) and semantic transparency (ST; evaluating the extent to which the compound meaning is recoverable from the constituents\u2019 semantics). We show that BERT-based measures moderately align with human intuitions, especially when using contextualized representations, and that LMD is overall more predictable than ST. Contrary to the results reported for \u2018standard\u2019 words, higher, more contextualized layers are the best at representing compound meaning. These findings shed new light on the abilities of BERT in dealing with fine-grained semantic phenomena. Moreover, they can provide insights into how speakers represent compounds.", "citations": 13}
{"title": "Probing for idiomaticity in vector space models", "year": 2021, "authors": "Marcos Garcia, Tiago Kramer Vieira, Carolina Scarton, M. Idiart, Aline Villavicencio", "url": "https://api.semanticscholar.org/CorpusId:233189591", "relevance": 3, "abstract": "Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sensitivity to lexical choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models", "citations": 61}
{"title": "Syntax-guided Neural Module Distillation to Probe Compositionality in Sentence Embeddings", "year": 2023, "authors": "Rohan Pandey", "url": "https://api.semanticscholar.org/CorpusId:256105273", "relevance": 2, "abstract": "Past work probing compositionality in sentence embedding models faces issues determining the causal impact of implicit syntax representations. Given a sentence, we construct a neural module net based on its syntax parse and train it end-to-end to approximate the sentence\u2019s embedding generated by a transformer model. The distillability of a transformer to a Syntactic NeurAl Module Net (SynNaMoN) then captures whether syntax is a strong causal model of its compositional ability. Furthermore, we address questions about the geometry of semantic composition by specifying individual SynNaMoN modules\u2019 internal architecture & linearity. We find differences in the distillability of various sentence embedding models that broadly correlate with their performance, but observe that distillability doesn\u2019t considerably vary by model size. We also present preliminary evidence that much syntax-guided composition in sentence embedding models is linear, and that non-linearities may serve primarily to handle non-compositional phrases.", "citations": 1}
{"title": "Assessing Phrasal Representation and Composition in Transformers", "year": 2020, "authors": "Lang-Chi Yu, Allyson Ettinger", "url": "https://www.semanticscholar.org/paper/a129bed0b735918da1c797a27ebb43a541971c64", "relevance": 1, "abstract": "Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.", "citations": 71}
{"title": "Semantics of Multiword Expressions in Transformer-Based Models: A Survey", "year": 2024, "authors": "Filip Miletic, Sabine Schulte im Walde", "url": "https://api.semanticscholar.org/CorpusId:267311998", "relevance": 1, "abstract": "Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.", "citations": 14}
{"title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis", "year": 2025, "authors": "Aishik Nagar, Ishaan Singh Rawal, Mansi Dhanania, Cheston Tan", "url": "https://api.semanticscholar.org/CorpusId:279075928", "relevance": 1, "abstract": "Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.", "citations": 0}
{"title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition", "year": 2019, "authors": "Vered Shwartz, Ido Dagan", "url": "https://www.semanticscholar.org/paper/a5ec883e080d858b5df60f1b5d711d514459b1e4", "relevance": 1, "abstract": "Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.", "citations": 86}
{"title": "Semantic Composition in Visually Grounded Language Models", "year": 2023, "authors": "Rohan Pandey", "url": "https://api.semanticscholar.org/CorpusId:258947627", "relevance": 1, "abstract": "What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically&relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether&how models compose visually grounded semantics, and how we might improve their ability to do so. Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital for vision-language composition, 4) Syntactic MeanPool to inject a compositional inductive bias into sentence embeddings, and 5) Cross-modal Attention Congruence Regularization, a self-supervised objective function for vision-language relation alignment. We close by discussing connections of our work to neuroscience, psycholinguistics, formal semantics, and philosophy.", "citations": 1}
{"title": "Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?", "year": 2024, "authors": "Vasudevan Nedumpozhimana, John Kelleher", "url": "https://api.semanticscholar.org/CorpusId:268248906", "relevance": 1, "abstract": "\n Transformer-based neural language models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models\u2019 (BERT and RoBERTa\u2019s) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers, but also that the facility of these models to distinguish idiomatic usage is primarily based on their ability to identify and encode topic. Furthermore, our analysis of these models\u2019 performance on other standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for these models.", "citations": 2}
{"title": "A Systematic Search for Compound Semantics in Pretrained BERT Architectures", "year": 2023, "authors": "Filip Miletic, Sabine Schulte im Walde", "url": "https://api.semanticscholar.org/CorpusId:258378347", "relevance": 1, "abstract": "To date, transformer-based models such as BERT have been less successful in predicting compositionality of noun compounds than static word embeddings. This is likely related to a suboptimal use of the encoded information, reflecting an incomplete grasp of how the models represent the meanings of complex linguistic structures. This paper investigates variants of semantic knowledge derived from pretrained BERT when predicting the degrees of compositionality for 280 English noun compounds associated with human compositionality ratings. Our performance strongly improves on earlier unsupervised implementations of pretrained BERT and highlights beneficial decisions in data preprocessing, embedding computation, and compositionality estimation. The distinct linguistic roles of heads and modifiers are reflected by differences in BERT-derived representations, with empirical properties such as frequency, productivity, and ambiguity affecting model performance. The most relevant representational information is concentrated in the initial layers of the model architecture.", "citations": 14}
{"title": "Characterizing Intrinsic Compositionality in Transformers with Tree Projections", "year": 2022, "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning", "url": "https://www.semanticscholar.org/paper/5116af9e9265044d0919111f1e0e1eb283de3a1f", "relevance": 1, "abstract": "When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, tree-like computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can route information arbitrarily between different parts of their input. One possibility is that these models, while extremely flexible in principle, in practice learn to interpret language hierarchically, ultimately building sentence representations close to those predictable by a bottom-up, tree-structured model. To evaluate this possibility, we describe an unsupervised and parameter-free method to \\emph{functionally project} the behavior of any transformer into the space of tree-structured networks. Given an input sentence, we produce a binary tree that approximates the transformer's representation-building process and a score that captures how\"tree-like\"the transformer's behavior is on the input. While calculation of this score does not require training any additional models, it provably upper-bounds the fit between a transformer and any tree-structured approximation. Using this method, we show that transformers for three different tasks become more tree-like over the course of training, in some cases unsupervisedly recovering the same trees as supervised parsers. These trees, in turn, are predictive of model behavior, with more tree-like models generalizing better on tests of compositional generalization.", "citations": 47}
{"title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "year": 2025, "authors": "Yuval Reif, Guy Kaplan, Roy Schwartz", "url": "https://api.semanticscholar.org/CorpusId:282209269", "relevance": 1, "abstract": "Large language models (LLMs) were shown to encode word form variations, such as\"walk\"->\"walked\", as linear directions in embedding space. However, standard tokenization algorithms treat these variations as distinct tokens -- filling the size-capped vocabulary with surface form variants (e.g.,\"walk\",\"walking\",\"Walk\"), at the expense of less frequent words and multilingual coverage. We show that many of these variations can be captured by transformation vectors -- additive offsets that yield the appropriate word's representation when applied to the base form word embedding -- in both the input and output spaces. Building on this, we propose a compact reshaping of the vocabulary: rather than assigning unique tokens to each surface form, we compose them from shared base form and transformation vectors (e.g.,\"walked\"=\"walk\"+ past tense). We apply our approach to multiple LLMs and across five languages, removing up to 10% of vocabulary entries -- thereby freeing space to allocate new, more diverse tokens. Importantly, we do so while also expanding vocabulary coverage to out-of-vocabulary words, with minimal impact on downstream performance, and without modifying model weights. Our findings motivate a foundational rethinking of vocabulary design, moving from string enumeration to a compositional vocabulary that leverages the underlying structure of language.", "citations": 0}
{"title": "Investigating Idiomaticity in Word Representations", "year": 2024, "authors": "Wei He, Tiago Kramer Vieira, Marcos Garc\u00eda, Carolina Scarton, M. Idiart, Aline Villavicencio", "url": "https://api.semanticscholar.org/CorpusId:273821248", "relevance": 1, "abstract": "\n Idiomatic expressions are an integral part of human languages, often used to express complex ideas in compressed or conventional ways (e.g. eager beaver as a keen and enthusiastic person). However, their interpretations may not be straightforwardly linked to the meanings of their individual components in isolation and this may have an impact for compositional approaches. In this paper, we investigate to what extent word representation models are able to go beyond compositional word combinations and capture multiword expression idiomaticity and some of the expected properties related to idiomatic meanings. We focus on noun compounds of varying levels of idiomaticity in two languages (English and Portuguese), presenting a dataset of minimal pairs containing human idiomaticity judgments for each noun compound at both type and token levels, their paraphrases and their occurrences in naturalistic and sense-neutral contexts, totalling 32,200 sentences. We propose this set of minimal pairs for evaluating how well a model captures idiomatic meanings, and define a set of fine-grained metrics of Affinity and Scaled Similarity, to determine how sensitive the models are to perturbations that may lead to changes in idiomaticity. Affinity is a comparative measure of the similarity between an experimental item, a target and a potential distractor, and Scaled Similarity incorporates a rescaling factor to magnify the meaningful similarities within the spaces defined by each specific model. The results obtained with a variety of representative and widely used models indicate that, despite superficial indications to the contrary in the form of high similarities, idiomaticity is not yet accurately represented in current models. Moreover, the performance of models with different levels of contextualisation suggests that their ability to capture context is not yet able to go beyond more superficial lexical clues provided by the words and to actually incorporate the relevant semantic clues needed for idiomaticity. By proposing model-agnostic measures for assessing the ability of models to capture idiomaticity, this paper contributes to determining limitations in the handling of non-compositional structures, which is one of the directions that needs to be considered for more natural, accurate and robust language understanding. The source code and additional materials related to this paper are available at our GitHub repository.", "citations": 1}
{"title": "On the Interplay Between Fine-tuning and Composition in Transformers", "year": 2021, "authors": "Lang-Chi Yu, Allyson Ettinger", "url": "https://www.semanticscholar.org/paper/52b1cf563d1368f72e82b91b0349a7012a746f4f", "relevance": 1, "abstract": "Pre-trained transformer language models have shown remarkable performance on a variety of NLP tasks. However, recent research has suggested that phrase-level representations in these models reflect heavy influences of lexical content, but lack evidence of sophisticated, compositional phrase information. Here we investigate the impact of fine-tuning on the capacity of contextualized embeddings to capture phrase meaning information beyond lexical content. Specifically, we fine-tune models on an adversarial paraphrase classification task with high lexical overlap, and on a sentiment classification task. After fine-tuning, we analyze phrasal representations in controlled settings following prior work. We find that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models. In follow-up analyses, we identify confounding cues in the paraphrase dataset that may explain the lack of composition benefits from that task, and we discuss potential factors underlying the localized benefits from sentiment training.", "citations": 14}
{"title": "Quantum NLP models on Natural Language Inference", "year": 2025, "authors": "Ling Sun, Peter Sullivan, Michael Martin, Yun Zhou", "url": "https://api.semanticscholar.org/CorpusId:282209826", "relevance": 1, "abstract": "Quantum natural language processing (QNLP) offers a novel approach to semantic modeling by embedding compositional structure directly into quantum circuits. This paper investigates the application of QNLP models to the task of Natural Language Inference (NLI), comparing quantum, hybrid, and classical transformer-based models under a constrained few-shot setting. Using the lambeq library and the DisCoCat framework, we construct parameterized quantum circuits for sentence pairs and train them for both semantic relatedness and inference classification. To assess efficiency, we introduce a novel information-theoretic metric, Information Gain per Parameter (IGPP), which quantifies learning dynamics independent of model size. Our results demonstrate that quantum models achieve performance comparable to classical baselines while operating with dramatically fewer parameters. The Quantum-based models outperform randomly initialized transformers in inference and achieve lower test error on relatedness tasks. Moreover, quantum models exhibit significantly higher per-parameter learning efficiency (up to five orders of magnitude more than classical counterparts), highlighting the promise of QNLP in low-resource, structure-sensitive settings. To address circuit-level isolation and promote parameter sharing, we also propose a novel cluster-based architecture that improves generalization by tying gate parameters to learned word clusters rather than individual tokens.", "citations": 0}
{"title": "Montague semantics and modifier consistency measurement in neural language models", "year": 2022, "authors": "Danilo S. Carvalho, Edoardo Manino, Julia Rozanova, Lucas C. Cordeiro, Andr\u00e9 Freitas", "url": "https://api.semanticscholar.org/CorpusId:254408575", "relevance": 1, "abstract": "This work proposes a novel methodology for measuring compositional behavior in contemporary language embedding models. Specifically, we focus on adjectival modifier phenomena in adjective-noun phrases. In recent years, distributional language representation models have demonstrated great practical success. At the same time, the need for interpretability has elicited questions on their intrinsic properties and capabilities. Crucially, distributional models are often inconsistent when dealing with compositional phenomena in natural language, which has significant implications for their safety and fairness. Despite this, most current research on compositionality is directed towards improving their performance on similarity tasks only. This work takes a different approach, introducing three novel tests of compositional behavior inspired by Montague semantics. Our experimental results indicate that current neural language models do not behave according to the expected linguistic theories. This indicates that current language models may lack the capability to capture the semantic properties we evaluated on limited context, or that linguistic theories from Montagovian tradition may not match the expected capabilities of distributional models.", "citations": 0}
{"title": "Using Shapley interactions to understand how models use structure", "year": 2024, "authors": "Divyansh Singhvi, Diganta Misra, Andrej Erkelens, Raghav Jain, Isabel Papadimitriou, Naomi Saphra", "url": "https://api.semanticscholar.org/CorpusId:268536933", "relevance": 1, "abstract": "Language is an intricately structured system, and a key goal of NLP interpretability is to provide methodological insights for understanding how language models represent this structure internally. In this paper, we use Shapley Taylor interaction indices (STII) in order to examine how language and speech models internally relate and structure their inputs. Pairwise Shapley interactions measure how much two inputs work together to influence model outputs beyond if we linearly added their independent influences, providing a view into how models encode structural interactions between inputs. We relate the interaction patterns in models to three underlying linguistic structures: syntactic structure, non-compositional semantics, and phonetic coarticulation. We find that autoregressive text models encode interactions that correlate with the syntactic proximity of inputs, and that both autoregressive and masked models encode nonlinear interactions in idiomatic phrases with non-compositional semantics. Our speech results show that inputs are more entangled for pairs where a neighboring consonant is likely to influence a vowel or approximant, showing that models encode the phonetic interaction needed for extracting discrete phonemic representations.", "citations": 1}
{"title": "Systematicity in GPT-3's Interpretation of Novel English Noun Compounds", "year": 2022, "authors": "Siyan Li, Riley Carlson, Christopher Potts", "url": "https://www.semanticscholar.org/paper/74be37384adc9b643b0c0a2d3b26c1361c5d779b", "relevance": 1, "abstract": "Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the same interpretive principles? To address this question, we first compare Levin et al.'s experimental data with GPT-3 generations, finding a high degree of similarity. However, this evidence is consistent with GPT3 reasoning only about specific lexical items rather than the more abstract conceptual categories of Levin et al.'s theory. To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning. Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items. These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory.", "citations": 16}
{"title": "Conceptual Combination in Large Language Models: Uncovering Implicit Relational Interpretations in Compound Words With Contextualized Word Embeddings", "year": 2025, "authors": "Marco Ciapparelli, Calogero Zarbo, Marco Marelli", "url": "https://www.semanticscholar.org/paper/6ea035cdf405af744ec0693878f67453ace2ba05", "relevance": 1, "abstract": "Large language models (LLMs) have been proposed as candidate models of human semantics, and as such, they must be able to account for conceptual combination. This work explores the ability of two LLMs, namely, BERT-base and Llama-2-13b, to reveal the implicit meaning of existing and novel compound words. According to psycholinguistic theories, understanding the meaning of a compound (e.g., \"snowman\") involves its automatic decomposition into constituent meanings (\"snow,\" \"man\"), which are then connected by an implicit semantic relation selected from a set of possible competitors (FOR, MADE OF, BY, \u2026) to obtain a plausible interpretation (\"man MADE OF snow\"). Here, we leverage the flexibility of LLMs to obtain contextualized representations for both target compounds (e.g., \"snowman\") and their implicit interpretations (e.g., \"man MADE OF snow\"). We demonstrate that replacing a compound with a paraphrased version leads to changes to the embeddings that are inversely proportional to the paraphrase's plausibility, estimated by human raters. While this relation holds for both existing and novel compounds, results obtained for novel compounds are substantially weaker, and older distributional models outperform LLMs. Nonetheless, the present results show that LLMs can offer a valid approximation of the internal structure of compound words posited by cognitive theories, thus representing a promising tool to model word senses that are at once implicit and possible.", "citations": 2}
{"title": "Emergent linguistic structure in artificial neural networks trained by self-supervision", "year": 2020, "authors": "Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, Omer Levy", "url": "https://api.semanticscholar.org/CorpusId:219315567", "relevance": 1, "abstract": "This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.", "citations": 370}
{"title": "Knowledge-Aware Language Model Pretraining", "year": 2020, "authors": "Corby Rosset, Chenyan Xiong, M. Phan, Xia Song, Paul N. Bennett, Saurabh Tiwary", "url": "https://api.semanticscholar.org/CorpusId:220280797", "relevance": 1, "abstract": "How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entity-extended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in the hidden representations through edge probing.We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.", "citations": 88}
{"title": "Vy\u0101karana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages", "year": 2021, "authors": "Rajaswa Patil, Jasleen Dhillon, Siddhant Mahurkar, Saumitra Kulkarni, M. Malhotra, V. Baths", "url": "https://api.semanticscholar.org/CorpusId:232075716", "relevance": 1, "abstract": "While there has been significant progress towards developing NLU resources for Indic languages, syntactic evaluation has been relatively less explored. Unlike English, Indic languages have rich morphosyntax, grammatical genders, free linear word-order, and highly inflectional morphology. In this paper, we introduce Vy\u0101karana: a benchmark of Colorless Green sentences in Indic languages for syntactic evaluation of multilingual language models. The benchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the datasets from the evaluation tasks to probe five multilingual language models of varying architectures for syntax in Indic languages. Due to its prevalence, we also include a code-switching setting in our experiments. Our results show that the token-level and sentence-level representations from the Indic language models (IndicBERT and MuRIL) do not capture the syntax in Indic languages as efficiently as the other highly multilingual language models. Further, our layer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language models do not show such syntactic localization.", "citations": 2}
{"title": "Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers", "year": 2024, "authors": "Cl\u00e9ment Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West", "url": "https://api.semanticscholar.org/CorpusId:273993540", "relevance": 1, "abstract": "A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word-translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean representation of a concept across different languages does not affect the models'ability to translate it, but instead improves it. Finally, we generalize to multi-token generation and demonstrate that the model can generate natural language description of those mean representations. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.", "citations": 21}
{"title": "Linguistic Knowledge and Transferability of Contextual Representations", "year": 2019, "authors": "Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith", "url": "https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975", "relevance": 1, "abstract": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.", "citations": 794}
{"title": "Implicit Representations of Meaning in Neural Language Models", "year": 2021, "authors": "Belinda Z. Li, Maxwell Nye, Jacob Andreas", "url": "https://api.semanticscholar.org/CorpusId:235294296", "relevance": 1, "abstract": "Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity\u2019s current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.", "citations": 215}
{"title": "Hidden Schema Networks", "year": 2022, "authors": "Rams\u00e9s J. S\u00e1nchez, L. Conrads, Pascal Welke, K. Cvejoski, C. Ojeda", "url": "https://api.semanticscholar.org/CorpusId:250408106", "relevance": 1, "abstract": "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.", "citations": 5}
{"title": "BERT Rediscovers the Classical NLP Pipeline", "year": 2019, "authors": "Ian Tenney, Dipanjan Das, Ellie Pavlick", "url": "https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c", "relevance": 1, "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.", "citations": 1694}
{"title": "What Does BERT Learn about the Structure of Language?", "year": 2019, "authors": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah", "url": "https://api.semanticscholar.org/CorpusId:195477534", "relevance": 1, "abstract": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT\u2019s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.", "citations": 1452}
{"title": "Towards Equipping Transformer with the Ability of Systematic Compositionality", "year": 2023, "authors": "Chen Huang, Peixin Qin, Wenqiang Lei, Jiancheng Lv", "url": "https://api.semanticscholar.org/CorpusId:266174049", "relevance": 1, "abstract": "One of the key factors in language productivity and human cognition is the ability of Systematic Compositionality, which refers to understanding composed, unseen examples of seen primitives. However, recent evidence reveals that the Transformers have difficulty in generalizing the composed context based on the seen primitives. To this end, we take the first step to propose a compositionality-aware Transformer called CAT and two novel pre-training tasks to facilitate the systematic compositionality. We tentatively provide a successful implementation of a multi-layer CAT on the basis of the especially popular BERT. The experimental results demonstrate that CAT outperforms baselines on compositionality-aware tasks with minimal impact on effectiveness on standardized language understanding tasks.", "citations": 2}
{"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "year": 2019, "authors": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick", "url": "https://api.semanticscholar.org/CorpusId:108300988", "relevance": 1, "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "citations": 922}
{"title": "How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks", "year": 2022, "authors": "Bingzhi Li, Guillaume Wisniewski, Beno\u00eet Crabb\u00e9", "url": "https://api.semanticscholar.org/CorpusId:248780296", "relevance": 1, "abstract": "This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar.", "citations": 5}
{"title": "Success and failure of compositional generalisation in distributional models of language", "year": 2025, "authors": "Shufan Mao, Philip A. Huebner, Jon Willits", "url": "https://api.semanticscholar.org/CorpusId:275288403", "relevance": 1, "abstract": "ABSTRACT Are distributional learning mechanisms capable of complex linguistic inferences requiring compositional generalisation? This question has become contentious with the development of large language models, which mimic human language abilities in many ways, but which struggle with compositional generalisation. We investigated a set of qualitatively different distributional models (word co-occurrence models, graphical models, recurrent neural networks, and Transformers), by training them on a carefully controlled artificial language containing combinatorial dependencies involving multiple words, and then testing them on novel sequences containing distributionally overlapping combinatorial dependencies. In this work, we show that graphical network models and Transformers, but not co-occurrence space models and recurrent neural networks, were able to perform compositional generalisation. This work demonstrates that the kinds of distributional models that can perform compositional generalisation are those that can represent words both individually and as a part of the phrases in which they participate.", "citations": 0}
{"title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models", "year": 2021, "authors": "Zeming Chen, Qiyue Gao", "url": "https://api.semanticscholar.org/CorpusId:244896159", "relevance": 1, "abstract": "Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.", "citations": 11}
{"title": "A Primer in BERTology: What We Know About How BERT Works", "year": 2020, "authors": "Anna Rogers, Olga Kovaleva, Anna Rumshisky", "url": "https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0", "relevance": 1, "abstract": "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.", "citations": 1752}
{"title": "Compositional Distributional Semantics with Syntactic Dependencies and Selectional Preferences", "year": 2021, "authors": "Pablo Gamallo", "url": "https://api.semanticscholar.org/CorpusId:237871824", "relevance": 1, "abstract": "This article describes a compositional model based on syntactic dependencies which has been designed to build contextualized word vectors, by following linguistic principles related to the concept of selectional preferences. The compositional strategy proposed in the current work has been evaluated on a syntactically controlled and multilingual dataset, and compared with Transformer BERT-like models, such as Sentence BERT, the state-of-the-art in sentence similarity. For this purpose, we created two new test datasets for Portuguese and Spanish on the basis of that defined for the English language, containing expressions with noun-verb-noun transitive constructions. The results we have obtained show that the linguistic-based compositional approach turns out to be competitive with Transformer models.", "citations": 6}
{"title": "StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure", "year": 2023, "authors": "Mattia Opper, Victor Prokhorov, N. Siddharth", "url": "https://api.semanticscholar.org/CorpusId:258564241", "relevance": 1, "abstract": "This work presents StrAE: a Structured Autoencoder framework that through strict adherence to explicit structure, and use of a novel contrastive objective over tree-structured representations, enables effective learning of multi-level representations. Through comparison over different forms of structure, we verify that our results are directly attributable to the informativeness of the structure provided as input, and show that this is not the case for existing tree models. We then further extend StrAE to allow the model to define its own compositions using a simple localised-merge algorithm. This variant, called Self-StrAE, outperforms baselines that don't involve explicit hierarchical compositions, and is comparable to models given informative structure (e.g. constituency parses). Our experiments are conducted in a data-constrained (circa 10M tokens) setting to help tease apart the contribution of the inductive bias to effective learning. However, we find that this framework can be robust to scale, and when extended to a much larger dataset (circa 100M tokens), our 430 parameter model performs comparably to a 6-layer RoBERTa many orders of magnitude larger in size. Our findings support the utility of incorporating explicit composition as an inductive bias for effective representation learning.", "citations": 2}
{"title": "Augmenting transformers with recursively composed multi-grained representations", "year": 2023, "authors": "Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu", "url": "https://api.semanticscholar.org/CorpusId:263131555", "relevance": 1, "abstract": "We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. We conduct experiments on various sentence-level and span-level tasks. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all span-level tasks and baselines that combine recursive networks with Transformers on natural language inference tasks. More interestingly, the hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers.", "citations": 5}
{"title": "HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword Expressions using Multilingual Pretrained Language Models", "year": 2022, "authors": "Minghuan Tan", "url": "https://api.semanticscholar.org/CorpusId:249152114", "relevance": 1, "abstract": "This paper describes an approach to detect idiomaticity only from the contextualized representation of a MWE over multilingual pretrained language models.Our experiments find that larger models are usually more effective in idiomaticity detection. However, using a higher layer of the model may not guarantee a better performance.In multilingual scenarios, the convergence of different languages are not consistent and rich-resource languages have big advantages over other languages.", "citations": 0}
{"title": "Contextualized word senses: from attention to compositionality", "year": 2023, "authors": "Pablo Gamallo", "url": "https://api.semanticscholar.org/CorpusId:265520130", "relevance": 1, "abstract": "Abstract The neural architectures of language models are becoming increasingly complex, especially that of Transformers, based on the attention mechanism. Although their application to numerous natural language processing tasks has proven to be very fruitful, they continue to be models with little or no interpretability and explainability. One of the tasks for which they are best suited is the encoding of the contextual sense of words using contextualized embeddings. In this paper we propose a transparent, interpretable, and linguistically motivated strategy for encoding the contextual sense of words by modeling semantic compositionality. Particular attention is given to dependency relations and semantic notions such as selection preferences and paradigmatic classes. A partial implementation of the proposed model is carried out and compared with Transformer-based architectures for a given semantic task, namely the similarity calculation of word senses in context. The results obtained show that it is possible to be competitive with linguistically motivated models instead of using the black boxes underlying complex neural architectures.", "citations": 2}
{"title": "The Dual-Route Model of Induction", "year": 2025, "authors": "Sheridan Feucht, Eric Todd, Byron C. Wallace, David Bau", "url": "https://api.semanticscholar.org/CorpusId:277596076", "relevance": 1, "abstract": "Prior work on in-context copying has shown the existence of induction heads, which attend to and promote individual tokens during copying. In this work we discover a new type of induction head: concept-level induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim (like copying nonsense tokens). These two\"routes\"operate independently: we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. By patching concept induction head outputs, we find that they contain language-independent word representations that mediate natural language translation, suggesting that LLMs represent abstract word meanings independent of language or form.", "citations": 14}
{"title": "Unsupervised Compositionality Prediction of Nominal Compounds", "year": 2019, "authors": "S. Cordeiro, Aline Villavicencio, M. Idiart, Carlos Ramisch", "url": "https://api.semanticscholar.org/CorpusId:56595638", "relevance": 1, "abstract": "Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.", "citations": 67}
{"title": "Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds", "year": 2024, "authors": "Giulia Rambelli, Emmanuele Chersoni, Claudia Collacciani, Marianna Bolognesi", "url": "https://api.semanticscholar.org/CorpusId:271923517", "relevance": 1, "abstract": "Noun-noun compounds interpretation is the task where a model is given one of such constructions, and it is asked to provide a para-phrase, making the semantic relation between the nouns explicit, as in carrot cake is \u201ca cake made of carrots.\u201d Such a task requires the ability to understand the implicit structured representation of the compound meaning. In this paper, we test to what extent the recent Large Language Models can interpret the semantic relation between the constituents of lexicalized English compounds and whether they can abstract from such semantic knowledge to predict the semantic relation between the constituents of similar but novel compounds by relying on analogical comparisons (e.g., carrot dessert ). We test both Sur-prisal metrics and prompt-based methods to see whether i.) they can correctly predict the relation between constituents, and ii.) the semantic representation of the relation is robust to paraphrasing. Using a dataset of lexicalized and annotated noun-noun compounds, we find that LLMs can infer some semantic relations better than others (with a preference for compounds involving concrete concepts). When challenged to perform abstractions and transfer their interpretations to semantically similar but novel compounds, LLMs show serious limitations 1 .", "citations": 7}
{"title": "MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition", "year": 2023, "authors": "Guangyue Xu, Parisa Kordjamshidi, Joyce Chai", "url": "https://api.semanticscholar.org/CorpusId:265019150", "relevance": 1, "abstract": "Humans have the ability to learn novel compositional concepts by recalling and generalizing primitive concepts acquired from past experiences. Inspired by this observation, in this paper, we propose MetaReVision, a retrieval-enhanced meta-learning model to address the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as a supporting set to meta-train vision-anguage models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel compositional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show that MetaReVision outperforms other competitive baselines and the retrieval module plays an important role in this compositional learning process.", "citations": 2}
{"title": "Shapley Idioms: Analysing BERT Sentence Embeddings for General Idiom Token Identification", "year": 2022, "authors": "Vasudevan Nedumpozhimana, Filip Klubicka, John D. Kelleher", "url": "https://www.semanticscholar.org/paper/bd948e4405183df24a35af2f2edca7dd138a7f62", "relevance": 1, "abstract": "This article examines the basis of Natural Language Understanding of transformer based language models, such as BERT. It does this through a case study on idiom token classification. We use idiom token identification as a basis for our analysis because of the variety of information types that have previously been explored in the literature for this task, including: topic, lexical, and syntactic features. This variety of relevant information types means that the task of idiom token identification enables us to explore the forms of linguistic information that a BERT language model captures and encodes in its representations. The core of this article presents three experiments. The first experiment analyzes the effectiveness of BERT sentence embeddings for creating a general idiom token identification model and the results indicate that the BERT sentence embeddings outperform Skip-Thought. In the second and third experiment we use the game theory concept of Shapley Values to rank the usefulness of individual idiomatic expressions for model training and use this ranking to analyse the type of information that the model finds useful. We find that a combination of idiom-intrinsic and topic-based properties contribute to an expression's usefulness in idiom token identification. Overall our results indicate that BERT efficiently encodes a variety of information from topic, through lexical and syntactic information. Based on these results we argue that notwithstanding recent criticisms of language model based semantics, the ability of BERT to efficiently encode a variety of linguistic information types does represent a significant step forward in natural language understanding.", "citations": 15}
{"title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning", "year": 2015, "authors": "Jianpeng Cheng, Dimitri Kartsaklis", "url": "https://api.semanticscholar.org/CorpusId:30344", "relevance": 1, "abstract": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "citations": 74}
{"title": "Distributed representations for compositional semantics", "year": 2014, "authors": "Karl Moritz Hermann", "url": "https://api.semanticscholar.org/CorpusId:23273296", "relevance": 1, "abstract": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. \nThis dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP.", "citations": 14}
{"title": "Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions", "year": 2015, "authors": "Majid Yazdani, Meghdad Farahmand, James Henderson", "url": "https://api.semanticscholar.org/CorpusId:2546162", "relevance": 1, "abstract": "Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation. We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered noncompositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.", "citations": 47}
{"title": "A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions", "year": 2015, "authors": "Bahar Salehi, Paul Cook, Timothy Baldwin", "url": "https://api.semanticscholar.org/CorpusId:203279", "relevance": 1, "abstract": "This paper presents the first attempt to use word embeddings to predict the compositionality of multiword expressions. We consider both single- and multi-prototype word embeddings. Experimental results show that, in combination with a back-off method based on string similarity, word embeddings outperform a method using count-based distributional similarity. Our best results are competitive with, or superior to, state-of-the-art methods over three standard compositionality datasets, which include two types of multiword expressions and two languages.", "citations": 109}
{"title": "Deep Artificial Neural Networks Reveal a Distributed Cortical Network Encoding Propositional Sentence-Level Meaning", "year": 2021, "authors": "A. Anderson, Douwe Kiela, J. Binder, L. Fernandino, Colin J. Humphries, L. Conant, Rajeev D. S. Raizada, Scott Grimm, E. Lalor", "url": "https://www.semanticscholar.org/paper/78930497bf5d37e1768ebc44709eacbaa5d7727e", "relevance": 1, "abstract": "Understanding how and where in the brain sentence-level meaning is constructed from words presents a major scientific challenge. Recent advances have begun to explain brain activation elicited by sentences using vector models of word meaning derived from patterns of word co-occurrence in text corpora. These studies have helped map out semantic representation across a distributed brain network spanning temporal, parietal, and frontal cortex. However, it remains unclear whether activation patterns within regions reflect unified representations of sentence-level meaning, as opposed to superpositions of context-independent component words. This is because models have typically represented sentences as \u201cbags-of-words\u201d that neglect sentence-level structure. To address this issue, we interrogated fMRI activation elicited as 240 sentences were read by 14 participants (9 female, 5 male), using sentences encoded by a recurrent deep artificial neural-network trained on a sentence inference task (InferSent). Recurrent connections and nonlinear filters enable InferSent to transform sequences of word vectors into unified \u201cpropositional\u201d sentence representations suitable for evaluating intersentence entailment relations. Using voxelwise encoding modeling, we demonstrate that InferSent predicts elements of fMRI activation that cannot be predicted by bag-of-words models and sentence models using grammatical rules to assemble word vectors. This effect occurs throughout a distributed network, which suggests that propositional sentence-level meaning is represented within and across multiple cortical regions rather than at any single site. In follow-up analyses, we place results in the context of other deep network approaches (ELMo and BERT) and estimate the degree of unpredicted neural signal using an \u201cexperiential\u201d semantic model and cross-participant encoding. SIGNIFICANCE STATEMENT A modern-day scientific challenge is to understand how the human brain transforms word sequences into representations of sentence meaning. A recent approach, emerging from advances in functional neuroimaging, big data, and machine learning, is to computationally model meaning, and use models to predict brain activity. Such models have helped map a cortical semantic information-processing network. However, how unified sentence-level information, as opposed to word-level units, is represented throughout this network remains unclear. This is because models have typically represented sentences as unordered \u201cbags-of-words.\u201d Using a deep artificial neural network that recurrently and nonlinearly combines word representations into unified propositional sentence representations, we provide evidence that sentence-level information is encoded throughout a cortical network, rather than in a single region.", "citations": 53}
{"title": "Are Frequent Phrases Directly Retrieved like Idioms? An Investigation with Self-Paced Reading and Language Models", "year": 2023, "authors": "Giulia Rambelli, Emmanuele Chersoni, M. Senaldi, P. Blache, Alessandro Lenci", "url": "https://api.semanticscholar.org/CorpusId:258486918", "relevance": 1, "abstract": "An open question in language comprehension studies is whether non-compositional multiword expressions like idioms and compositional-but-frequent word sequences are processed differently. Are the latter constructed online, or are instead directly retrieved from the lexicon, with a degree of entrenchment depending on their frequency? In this paper, we address this question with two different methodologies. First, we set up a self-paced reading experiment comparing human reading times for idioms and both highfrequency and low-frequency compositional word sequences. Then, we ran the same experiment using the Surprisal metrics computed with Neural Language Models (NLMs). Our results provide evidence that idiomatic and high-frequency compositional expressions are processed similarly by both humans and NLMs. Additional experiments were run to test the possible factors that could affect the NLMs\u2019 performance.", "citations": 7}
{"title": "Asymmetric Underlying Mechanisms of Relation-Based and Property-Based Noun\u2013Noun Conceptual Combination", "year": 2021, "authors": "Mingyeong Choi, Sangsuk Yoon", "url": "https://api.semanticscholar.org/CorpusId:236323892", "relevance": 1, "abstract": "Conceptual combination is a fundamental human cognitive ability by which people can experience infinite thinking by artfully combining finite knowledge. For example, one can instantly combine \u201ccactus\u201d and \u201cfish\u201d together as \u201cprickly fish\u201d even if one has never previously heard of a \u201ccactus fish.\u201d Although two major combinatorial types\u2014property and relational combinations\u2014have been identified, the underlying processes of each remain elusive. This study investigates the asymmetric processing mechanisms underlying property and relational combinations by examining differential semantic activation during noun\u2013noun conceptual combination. Across two experiments utilizing each combinatorial process as semantic priming and implementing a lexical decision task immediately after combination, we measure and compare the semantic activation patterns of intrinsic and extrinsic semantic features in these two combinatorial types. We found converging evidence that property and relational combinations involve asymmetric semantic information and entail distinct processing mechanisms. In property combination, the intrinsic feature in the modifier concept showed greater activation than the semantic feature of the same dimension in the head concept. In contrast, in relational combination, the extrinsic semantic feature in the head concept and the whole modifier concept showed similar levels of activation. Moreover, our findings also showed that these patterns of semantic activation occurred only when the combinatorial process was complete, indicating that accessing the same lexical-semantic information is not sufficient to observe asymmetric patterns. These findings demonstrate that property combination involves replacing a specific semantic feature of the head noun with that of the modifier noun, whereas relational combination involves completing the semantic feature of the head noun with the whole modifier concept. We discuss the implications of these findings, research limitations, and future research directions.", "citations": 1}
{"title": "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?", "year": 2023, "authors": "Jordan Coil, Vered Shwartz", "url": "https://www.semanticscholar.org/paper/1cc45dd8a190a4a0aae44ddd100c797965853d78", "relevance": 1, "abstract": "Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds.", "citations": 19}
{"title": "Composition in Distributional Models of Semantics", "year": 2010, "authors": "Jeff Mitchell, Mirella Lapata", "url": "https://www.semanticscholar.org/paper/745d86adca56ec50761591733e157f84cfb19671", "relevance": 1, "abstract": "", "citations": 1027}
{"title": "The syntax and semantics of complex nominals", "year": 1978, "authors": "J. Levi", "url": "https://www.semanticscholar.org/paper/97208259ecbfc5d7039e0f171f4e5b05d116c5f7", "relevance": 1, "abstract": "", "citations": 776}
{"title": "RELPRON: A Relative Clause Evaluation Data Set for Compositional Distributional Semantics", "year": 2016, "authors": "Laura Rimell, Jean Maillard, T. Polajnar, S. Clark", "url": "https://api.semanticscholar.org/CorpusId:8992565", "relevance": 1, "abstract": "This article introduces RELPRON, a large data set of subject and object relative clauses, for the evaluation of methods in compositional distributional semantics. RELPRON targets an intermediate level of grammatical complexity between content-word pairs and full sentences. The task involves matching terms, such as \u201cwisdom,\u201d with representative properties, such as \u201cquality that experience teaches.\u201d A unique feature of RELPRON is that it is built from attested properties, but without the need for them to appear in relative clause format in the source corpus. The article also presents some initial experiments on RELPRON, using a variety of composition methods including simple baselines, arithmetic operators on vectors, and finally, more complex methods in which argument-taking words are represented as tensors. The latter methods are based on the Categorial framework, which is described in detail. The results show that vector addition is difficult to beat\u2014in line with the existing literature\u2014but that an implementation of the Categorial framework based on the Practical Lexical Function model is able to match the performance of vector addition. The article finishes with an in-depth analysis of RELPRON, showing how results vary across subject and object relative clauses, across different head nouns, and how the methods perform on the subtasks necessary for capturing relative clause semantics, as well as providing a qualitative analysis highlighting some of the more common errors. Our hope is that the competitive results presented here, in which the best systems are on average ranking one out of every two properties correctly for a given term, will inspire new approaches to the RELPRON ranking task and other tasks based on linguistically interesting constructions.", "citations": 34}
{"title": "An Empirical Study on Compositionality in Compound Nouns", "year": 2011, "authors": "Siva Reddy, Diana McCarthy, S. Manandhar", "url": "https://www.semanticscholar.org/paper/de17bdef43b2a7ac75447c494b9f791d951a6b27", "relevance": 1, "abstract": "", "citations": 163}
{"title": "Relation and lexical priming during the interpretation of noun-noun combinations.", "year": 2001, "authors": "Christina L. Gagn\u00e9", "url": "https://www.semanticscholar.org/paper/346247029eb830172bbd978038d687c1cfa99a2a", "relevance": 1, "abstract": "", "citations": 154}
{"title": "Embodied Conceptual Combination", "year": 2010, "authors": "Dermot Lynott, L. Connell", "url": "https://www.semanticscholar.org/paper/892af4c5c48673fe355cfbc4d6b992130fb18d35", "relevance": 1, "abstract": "Conceptual combination research investigates the processes involved in creating new meaning from old referents. It is therefore essential that embodied theories of cognition are able to explain this constructive ability and predict the resultant behavior. However, by failing to take an embodied or grounded view of the conceptual system, existing theories of conceptual combination cannot account for the role of perceptual, motor, and affective information in conceptual combination. In the present paper, we propose the embodied conceptual combination (ECCo) model to address this oversight. In ECCo, conceptual combination is the result of the interaction of the linguistic and simulation systems, such that linguistic distributional information guides or facilitates the combination process, but the new concept is fundamentally a situated, simulated entity. So, for example, a cactus beetle is represented as a multimodal simulation that includes visual (e.g., the shiny appearance of a beetle) and haptic (e.g., the prickliness of the cactus) information, all situated in the broader location of a desert environment under a hot sun, and with (at least for some people) an element of creepy-crawly revulsion. The ECCo theory differentiates interpretations according to whether the constituent concepts are destructively, or non-destructively, combined in the situated simulation. We compare ECCo to other theories of conceptual combination, and discuss how it accounts for classic effects in the literature.", "citations": 96}
{"title": "Leveraging Preposition Ambiguity to Assess Compositional Distributional Models of Semantics", "year": 2015, "authors": "Samuel Ritter, Cotie Long, Denis Paperno, Marco Baroni, M. Botvinick, A. Goldberg", "url": "https://api.semanticscholar.org/CorpusId:16334135", "relevance": 1, "abstract": "Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings. Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM have been difficult to interpret because the current metrics for assessing them do not control for the confound of lexical information. We present a new method for assessing the degree to which CDSM capture semantic interactions that dissociates the influences of lexical and compositional information. We then provide a dataset for performing this type of assessment and use it to evaluate six compositional models using both co-occurrence based and neural language model input vectors. Results show that neural language input vectors are consistently superior to co-occurrence based vectors, that several CDSM capture substantial compositional information, and that, surprisingly, vector addition matches and is in many cases superior to purpose-built paramaterized models.", "citations": 13}
{"title": "How Does the Left Anterior Temporal Lobe Contribute to Conceptual Combination? Interdisciplinary Perspectives", "year": 2017, "authors": "Masha Westerlund, L. Pylkk\u00e4nen", "url": "https://www.semanticscholar.org/paper/4004c50e15c84250f7400258fd93cb68c2b0d2c4", "relevance": 1, "abstract": "Within the cognitive neuroscience of language, the left anterior temporal lobe (LATL) is one of the most consistent loci for semantic effects; yet its precise role in semantic processing remains unclear. Here we focus on a literature showing that the LATL is modulated by semantic composition even in the simplest of cases, suggesting that it has a central role in the construction of complex meaning. We show that while the LATL\u2019s combinatory contribution generalizes across several linguistic factors, such as composition type and word order, it is also robustly modulated by conceptual factors such as the specificity of the composing words. These findings suggest that the LATL\u2019s role in composition is at the conceptual as opposed to the syntactic or logico-semantic level, making formal semantic theories of composition less obviously useful for guiding future research on the LATL. For an alternative theoretical foundation, this chapter seeks to connect LATL research to psychological models of conceptual combination, which potentially offer a more fruitful space of hypotheses to constrain our understanding of the computations housed in the LATL. We conclude that, though currently available data on the LATL do not rule out relation-based models, they are most consistent with schema-based models of conceptual combination, with the LATL potentially representing the site of concept schema activation and modification.", "citations": 10}
{"title": "Semantic Structure in Deep Learning", "year": 2021, "authors": "Ellie Pavlick", "url": "https://api.semanticscholar.org/CorpusId:244549257", "relevance": 1, "abstract": "Deep learning has recently come to dominate computational linguistics, leading to claims of human-level performance in a range of language processing tasks. Like much previous computational work, deep learning\u2013based linguistic representations adhere to the distributional meaning-in-use hypothesis, deriving semantic representations from word co-occurrence statistics. However, current deep learning methods entail fundamentally new models of lexical and compositional meaning that are ripe for theoretical analysis. Whereas traditional distributional semantics models take a bottom-up approach in which sentence meaning is characterized by explicit composition functions applied to word meanings, new approaches take a top-down approach in which sentence representations are treated as primary and representations of words and syntax are viewed as emergent. This article summarizes our current understanding of how well such representations capture lexical semantics, world knowledge, and composition. The goal is to foster increased collaboration on testing the implications of such representations as general-purpose models of semantics. Expected final online publication date for the Annual Review of Linguistics, Volume 8 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.", "citations": 62}
{"title": "Learning to Interpret Novel Noun-Noun Compounds: Evidence from Category Learning Experiments", "year": 2007, "authors": "Barry Devereux, F. Costello", "url": "https://www.semanticscholar.org/paper/4accaa422c3776ee72d6ceaf79906924f078bded", "relevance": 1, "abstract": "", "citations": 10}
{"title": "Unit Testing for Concepts in Neural Networks", "year": 2022, "authors": "Charles Lovering, Ellie Pavlick", "url": "https://api.semanticscholar.org/CorpusId:251719317", "relevance": 1, "abstract": "Abstract Many complex problems are naturally understood in terms of symbolic concepts. For example, our concept of \u201ccat\u201d is related to our concepts of \u201cears\u201d and \u201cwhiskers\u201d in a non-arbitrary way. Fodor (1998) proposes one theory of concepts, which emphasizes symbolic representations related via constituency structures. Whether neural networks are consistent with such a theory is open for debate. We propose unit tests for evaluating whether a system\u2019s behavior is consistent with several key aspects of Fodor\u2019s criteria. Using a simple visual concept learning task, we evaluate several modern neural architectures against this specification. We find that models succeed on tests of groundedness, modularity, and reusability of concepts, but that important questions about causality remain open. Resolving these will require new methods for analyzing models\u2019 internal states.", "citations": 30}
{"title": "Making Transformers Solve Compositional Tasks", "year": 2021, "authors": "Santiago Ontan'on, J. Ainslie, V. Cvicek, Zachary Kenneth Fisher", "url": "https://www.semanticscholar.org/paper/49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2", "relevance": 1, "abstract": "Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).", "citations": 85}
{"title": "Linguistic generalization and compositionality in modern artificial neural networks", "year": 2019, "authors": "Marco Baroni", "url": "https://www.semanticscholar.org/paper/82e32585088ae5b8bf5497919f85022e397a75ad", "relevance": 1, "abstract": "In the last decade, deep artificial neural networks have achieved astounding performance in many natural language-processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language-processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language. This article is part of the theme issue \u2018Towards mechanistic models of meaning composition\u2019.", "citations": 157}
{"title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "year": 2025, "authors": "Yida Zhao, Hao Xve, Xiang Hu, Kewei Tu", "url": "https://api.semanticscholar.org/CorpusId:280010964", "relevance": 1, "abstract": "Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.", "citations": 1}
{"title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "year": 2025, "authors": "Yingji Zhang, Danilo S. Carvalho, Andr\u00e9 Freitas", "url": "https://api.semanticscholar.org/CorpusId:279999973", "relevance": 1, "abstract": "Integrating compositional and symbolic properties into current distributional semantic spaces can enhance the interpretability, controllability, compositionality, and generalisation capabilities of Transformer-based auto-regressive language models (LMs). In this survey, we offer a novel perspective on latent space geometry through the lens of compositional semantics, a direction we refer to as \\textit{semantic representation learning}. This direction enables a bridge between symbolic and distributional semantics, helping to mitigate the gap between them. We review and compare three mainstream autoencoder architectures-Variational AutoEncoder (VAE), Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the distinctive latent geometries they induce in relation to semantic structure and interpretability.", "citations": 0}
{"title": "COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models", "year": 2022, "authors": "Kanishka Misra, J. Rayz, Allyson Ettinger", "url": "https://api.semanticscholar.org/CorpusId:256697074", "relevance": 1, "abstract": "A characteristic feature of human semantic cognition is its ability to not only store and retrieve the properties of concepts observed through experience, but to also facilitate the inheritance of properties (can breathe) from superordinate concepts (animal) to their subordinates (dog)\u2014i.e. demonstrate property inheritance. In this paper, we present COMPS, a collection of minimal pair sentences that jointly tests pre-trained language models (PLMs) on their ability to attribute properties to concepts and their ability to demonstrate property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal that they can easily distinguish between concepts on the basis of a property when they are trivially different, but find it relatively difficult when concepts are related on the basis of nuanced knowledge representations. Furthermore, we find that PLMs can show behaviors suggesting successful property inheritance in simple contexts, but fail in the presence of distracting information, which decreases the performance of many models sometimes even below chance. This lack of robustness in demonstrating simple reasoning raises important questions about PLMs\u2019 capacity to make correct inferences even when they appear to possess the prerequisite knowledge.", "citations": 17}
{"title": "Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference", "year": 2025, "authors": "Go Kamoda, Benjamin Heinzerling, Tatsuro Inaba, Keito Kudo, Keisuke Sakaguchi, Kentaro Inui", "url": "https://api.semanticscholar.org/CorpusId:275921347", "relevance": 1, "abstract": "According to the stages-of-inference hypothesis, early layers of language models map their subword-tokenized input, which does not necessarily correspond to a linguistically meaningful segmentation, to more meaningful representations that form the model's\"inner vocabulary\". Prior analysis of this detokenization stage has predominantly relied on probing and interventions such as path patching, which involve selecting particular inputs, choosing a subset of components that will be patched, and then observing changes in model behavior. Here, we show that several important aspects of the detokenization stage can be understood purely by analyzing model weights, without performing any model inference steps. Specifically, we introduce an analytical decomposition of first-layer attention in GPT-2. Our decomposition yields interpretable terms that quantify the relative contributions of position-related, token-related, and mixed effects. By focusing on terms in this decomposition, we discover weight-based explanations of attention bias toward close tokens and attention for detokenization.", "citations": 4}
{"title": "Measuring and Narrowing the Compositionality Gap in Language Models", "year": 2022, "authors": "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, M. Lewis", "url": "https://www.semanticscholar.org/paper/e070ff286709db28312e08b52b05539debe88146", "relevance": 1, "abstract": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.", "citations": 990}
{"title": "Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models", "year": 2024, "authors": "Philipp Mondorf, Sondre Wold, Barbara Plank", "url": "https://api.semanticscholar.org/CorpusId:273026338", "relevance": 1, "abstract": "A fundamental question in interpretability research is to what extent neural networks, particularly language models, implement reusable functions through subnetworks that can be composed to perform more complex tasks. Recent advances in mechanistic interpretability have made progress in identifying $\\textit{circuits}$, which represent the minimal computational subgraphs responsible for a model's behavior on specific tasks. However, most studies focus on identifying circuits for individual tasks without investigating how functionally similar circuits $\\textit{relate}$ to each other. To address this gap, we study the modularity of neural networks by analyzing circuits for highly compositional subtasks within a transformer-based language model. Specifically, given a probabilistic context-free grammar, we identify and compare circuits responsible for ten modular string-edit operations. Our results indicate that functionally similar circuits exhibit both notable node overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits identified can be reused and combined through set operations to represent more complex functional model capabilities.", "citations": 2}
{"title": "Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning", "year": 2022, "authors": "Hao Zheng, Mirella Lapata", "url": "https://api.semanticscholar.org/CorpusId:254563860", "relevance": 1, "abstract": "Compositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We introduce two key modifications to this model which encourage more disentangled representations and improve its compute and memory efficiency, allowing us to tackle compositional generalization in a more realistic setting. Specifically, instead of adaptively re-encoding source keys and values at each time step, we disentangle their representations and only re-encode keys periodically, at some interval. Our new architecture leads to better generalization performance across existing tasks and datasets, and a new machine translation benchmark which we create by detecting naturally occurring compositional patterns in relation to a training set. We show this methodology better emulates real-world requirements than artificial challenges.", "citations": 6}
{"title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling", "year": 2024, "authors": "Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas", "url": "https://api.semanticscholar.org/CorpusId:268554093", "relevance": 1, "abstract": "Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.", "citations": 5}
{"title": "Why Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?", "year": 2022, "authors": "Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, Jaesik Choi", "url": "https://api.semanticscholar.org/CorpusId:251979676", "relevance": 1, "abstract": "Many contextualized word representations are now learned by intricate neural network models, such as masked neural language models (MNLMs) which are made up of huge neural network structures and trained to restore the masked text. Such representations demonstrate superhuman performance in some reading comprehension (RC) tasks which extract a proper answer in the context given a question. However, identifying the detailed knowledge trained in MNLMs is challenging owing to numerous and intermingled model parameters. This paper provides new insights and empirical analyses on commonsense knowledge included in pretrained MNLMs. First, we use a diagnostic test that evaluates whether commonsense knowledge is properly trained in MNLMs. We observe that a large proportion of commonsense knowledge is not appropriately trained in MNLMs and MNLMs do not often understand the semantic meaning of relations accurately. In addition, we find that the MNLM-based RC models are still vulnerable to semantic variations that require commonsense knowledge. Finally, we discover the fundamental reason why some knowledge is not trained. We further suggest that utilizing an external commonsense knowledge repository can be an effective solution. We exemplify the possibility to overcome the limitations of the MNLM-based RC models by enriching text with the required knowledge from an external commonsense knowledge repository in controlled experiments.", "citations": 0}
{"title": "Inducing Transformer\u2019s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks", "year": 2021, "authors": "Yichen Jiang, Mohit Bansal", "url": "https://api.semanticscholar.org/CorpusId:238227171", "relevance": 1, "abstract": "Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During inference, the model jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from \u2264 10% to 100%. With only 418 (5%) training instances, our approach still achieves 97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can be induced in Transformers given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention\u2019s query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).", "citations": 28}
{"title": "SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics", "year": 2021, "authors": "Hitomi Yanaka, K. Mineshima, Kentaro Inui", "url": "https://api.semanticscholar.org/CorpusId:235294010", "relevance": 1, "abstract": "Recently, deep neural networks (DNNs) have achieved great success in semantically challenging NLP tasks, yet it remains unclear whether DNN models can capture compositional meanings, those aspects of meaning that have been long studied in formal semantics. To investigate this issue, we propose a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena. Using SyGNS, we test whether neural networks can systematically parse sentences involving novel combinations of logical expressions such as quantifiers and negation. Experiments show that Transformer and GRU models can generalize to unseen combinations of quantifiers, negations, and modifiers that are similar to given training instances in form, but not to the others. We also find that the generalization performance to unseen combinations is better when the form of meaning representations is simpler. The data and code for SyGNS are publicly available at https://github.com/verypluming/SyGNS.", "citations": 11}
{"title": "Consistency Regularization Training for Compositional Generalization", "year": 2023, "authors": "Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Jie Zhou, Yue Zhang", "url": "https://api.semanticscholar.org/CorpusId:259370677", "relevance": 1, "abstract": "Existing neural models have difficulty generalizing to unseen combinations of seen components. To achieve compositional generalization, models are required to consistently interpret (sub)expressions across contexts. Without modifying model architectures, we improve the capability of Transformer on compositional generalization through consistency regularization training, which promotes representation consistency across samples and prediction consistency for a single sample. Experimental results on semantic parsing and machine translation benchmarks empirically demonstrate the effectiveness and generality of our method. In addition, we find that the prediction consistency scores on in-distribution validation sets can be an alternative for evaluating models during training, when commonly-used metrics are not informative.", "citations": 10}
{"title": "How GPT learns layer by layer", "year": 2025, "authors": "Jason Du, Kelly Hong, Alishba Imran, Erfan Jahanparast, Mehdi Khfifi, Kaichun Qiao", "url": "https://www.semanticscholar.org/paper/09792b696aab8003dbe1380b9dd06fb34e5d6779", "relevance": 1, "abstract": "Large Language Models (LLMs) excel at tasks like language processing, strategy games, and reasoning but struggle to build generalizable internal representations essential for adaptive decision-making in agents. For agents to effectively navigate complex environments, they must construct reliable world models. While LLMs perform well on specific benchmarks, they often fail to generalize, leading to brittle representations that limit their real-world effectiveness. Understanding how LLMs build internal world models is key to developing agents capable of consistent, adaptive behavior across tasks. We analyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a controlled testbed for studying representation learning. Despite being trained solely on next-token prediction with random valid moves, OthelloGPT shows meaningful layer-wise progression in understanding board state and gameplay. Early layers capture static attributes like board edges, while deeper layers reflect dynamic tile changes. To interpret these representations, we compare Sparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more robust, disentangled insights into compositional features, whereas linear probes mainly detect features useful for classification. We use SAEs to decode features related to tile color and tile stability, a previously unexamined feature that reflects complex gameplay concepts like board control and long-term planning. We study the progression of linear probe accuracy and tile color using both SAE's and linear probes to compare their effectiveness at capturing what the model is learning. Although we begin with a smaller language model, OthelloGPT, this study establishes a framework for understanding the internal representations learned by GPT models, transformers, and LLMs more broadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE.", "citations": 1}
{"title": "Iterative Decoding for Compositional Generalization in Transformers", "year": 2021, "authors": "Luana Ruiz, J. Ainslie, Santiago Ontan'on", "url": "https://api.semanticscholar.org/CorpusId:238531699", "relevance": 1, "abstract": "Deep learning models generalize well to in-distribution data but struggle to generalize compositionally, i.e., to combine a set of learned primitives to solve more complex tasks. In sequence-to-sequence (seq2seq) learning, transformers are often unable to predict correct outputs for longer examples than those seen at training. This paper introduces iterative decoding, an alternative to seq2seq that (i) improves transformer compositional generalization in the PCFG and Cartesian product datasets and (ii) evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled. In iterative decoding, training examples are broken down into a sequence of intermediate steps that the transformer learns iteratively. At inference time, the intermediate outputs are fed back to the transformer as intermediate inputs until an end-of-iteration token is predicted. We conclude by illustrating some limitations of iterative decoding in the CFQ dataset.", "citations": 7}
{"title": "Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors", "year": 2013, "authors": "Edward Grefenstette", "url": "https://api.semanticscholar.org/CorpusId:370914", "relevance": 1, "abstract": "The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate di erent aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors. It provides tensor interpretations of the set of logical connectives required to model propositional calculi. It suggests a variant of these tensor calculi capable of modelling quantifiers, using few non-linear operations. It finally discusses the relation between these variants, and how this relation should constitute the subject of future work.", "citations": 104}
{"title": "Category-theoretic quantitative compositional distributional models of natural language semantics", "year": 2013, "authors": "Edward Grefenstette", "url": "https://api.semanticscholar.org/CorpusId:20687969", "relevance": 1, "abstract": "This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text. \nThis thesis focuses on a particular approach to this compositionality problem, namely using the categorical framework developed by Coecke, Sadrzadeh, and Clark, which combines syntactic analysis formalisms with distributional semantic representations of meaning to produce syntactically motivated composition operations. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. It furthermore demonstrates that such models can perform on par with, or better than, other competing approaches in the field of natural language processing. \nThere are three principal contributions to computational linguistics in this thesis. The first is to extend the DisCoCat framework on the syntactic front and semantic front, incorporating a number of syntactic analysis formalisms and providing learning procedures allowing for the generation of concrete compositional distributional models. The second contribution is to evaluate the models developed from the procedures presented here, showing that they outperform other compositional distributional models present in the literature. The third contribution is to show how using category theory to solve linguistic problems forms a sound basis for research, illustrated by examples of work on this topic, that also suggest directions for future research.", "citations": 32}
{"title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding", "year": 2022, "authors": "H. T. Madabushi, Edward Gow-Smith, Marcos Garc\u00eda, Carolina Scarton, M. Idiart, Aline Villavicencio", "url": "https://api.semanticscholar.org/CorpusId:248299841", "relevance": 1, "abstract": "This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.", "citations": 67}
{"title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "year": 2025, "authors": "Frances Adriana Laureano De Leon, H. T. Madabushi, Mark G. Lee", "url": "https://api.semanticscholar.org/CorpusId:278171201", "relevance": 1, "abstract": "Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models.", "citations": 0}
{"title": "Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax", "year": 2025, "authors": "Iuliia Zaitova, Vitalii Hirak, Badr M. Abdullah, Dietrich Klakow, Bernd Mobius, T. Avgustinova", "url": "https://api.semanticscholar.org/CorpusId:278481469", "relevance": 1, "abstract": "This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages - English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.", "citations": 0}
{"title": "A Novel Approach: Tokenization Framework based on Sentence Structure in Indonesian Language", "year": 2023, "authors": "Johannes Petrus, Ermatita, Sukemi, Erwin", "url": "https://api.semanticscholar.org/CorpusId:257392014", "relevance": 1, "abstract": "\u2014This study proposes a new approach in the sentence tokenization process. Sentence tokenization, which is known so far, is the process of breaking sentences based on spaces as separators. Space-based sentence tokenization only generates single word tokens. In sentences consisting of five words, tokenization will produce five tokens, one word each. Each word is a token. This process ignores the loss of the original meaning of the separated words. Our proposed tokenization framework can generate one-word tokens and multi-word tokens at the same time. The process is carried out by extracting the sentence structure to obtain sentence elements. Each sentence element is a token. There are five sentence elements that is Subject, Predicate, Object, Complement and Adverbs. We extract sentence structures using deep learning methods, where models are built by training the datasets that have been prepared before. The training results are quite good with an F1 score of 0.7 and it is still possible to improve. Sentence similarity is the topic for measuring the performance of one-word tokens compared to multi-word tokens. In this case the multiword token has better accuracy. This framework was created using the Indonesian language but can also use other languages with dataset adjustments.", "citations": 2}
{"title": "Using large language models to estimate features of multi-word expressions: Concreteness, valence, arousal", "year": 2024, "authors": "Gonzalo Mart'inez, Juan Diego Molero, Sandra Gonz'alez, Javier Conde, M. Brysbaert, Pedro Reviriego", "url": "https://api.semanticscholar.org/CorpusId:272146403", "relevance": 1, "abstract": "This study investigates the potential of large language models (LLMs) to provide accurate estimates of concreteness, valence, and arousal for multi-word expressions. Unlike previous artificial intelligence (AI) methods, LLMs can capture the nuanced meanings of multi-word expressions. We systematically evaluated GPT-4o's ability to predict concreteness, valence, and arousal. In Study 1, GPT-4o showed strong correlations with human concreteness ratings (r = .8) for multi-word expressions. In Study 2, these findings were repeated for valence and arousal ratings of individual words, matching or outperforming previous AI models. Studies 3\u20135 extended the valence and arousal analysis to multi-word expressions and showed good validity of the LLM-generated estimates for these stimuli as well. To help researchers with stimulus selection, we provide datasets with LLM-generated norms of concreteness, valence, and arousal for 126,397 English single words and 63,680 multi-word expressions.", "citations": 22}
{"title": "Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels", "year": 2021, "authors": "Marcos Garc\u00eda, Tiago Kramer Vieira, Carolina Scarton, M. Idiart, Aline Villavicencio", "url": "https://api.semanticscholar.org/CorpusId:236459811", "relevance": 1, "abstract": "Accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. However, most existing resources with annotation of idiomaticity include ratings only at type level. This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in English and 180 in Portuguese at both type and token level. We compiled 8,725 and 5,091 token level annotations for English and Portuguese, respectively, which are strongly correlated with the corresponding scores obtained at type level. The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences. Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators. This new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels, with uses for lexical substitution or disambiguation in context.", "citations": 32}
{"title": "Improving Large Language Models with Concept-Aware Fine-Tuning", "year": 2025, "authors": "Michael Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao", "url": "https://api.semanticscholar.org/CorpusId:279251443", "relevance": 1, "abstract": "Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase\"ribonucleic acid\"as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\",\"on\", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm", "citations": 1}
{"title": "Building Models of Neurological Language", "year": 2025, "authors": "Henry Watkins", "url": "https://api.semanticscholar.org/CorpusId:279243766", "relevance": 1, "abstract": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.", "citations": 0}
{"title": "gaBERT \u2014 an Irish Language Model", "year": 2021, "authors": "James Barry, Joachim Wagner, Lauren Cassidy, Alan Cowap, Teresa Lynn, Abigail Walsh, M. J. \u00d3. Meachair, Jennifer Foster", "url": "https://api.semanticscholar.org/CorpusId:236447621", "relevance": 1, "abstract": "The BERT family of neural language models have become highly popular due to their ability to provide sequences of text with rich context-sensitive token encodings which are able to generalise well to many NLP tasks. We introduce gaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT model to multilingual BERT and the monolingual Irish WikiBERT, and we show that gaBERT provides better representations for a downstream parsing task. We also show how different filtering criteria, vocabulary size and the choice of subword tokenisation model affect downstream performance. We compare the results of fine-tuning a gaBERT model with an mBERT model for the task of identifying verbal multiword expressions, and show that the fine-tuned gaBERT model also performs better at this task. We release gaBERT and related code to the community.", "citations": 20}
{"title": "PARSEME-AR: Arabic reference corpus for multiword expressions using PARSEME annotation guidelines", "year": 2024, "authors": "Najet Hadj Mohamed, Ch\u00e9rifa Ben Khelil, Agata Savary, Iskander Keskes, Jean-Yves Antoine, Lamia Hadrich Belguith", "url": "https://api.semanticscholar.org/CorpusId:272227124", "relevance": 1, "abstract": "In this paper we present PARSEME-AR, the first openly available Arabic corpus manually annotated for Verbal Multiword Expressions (VMWEs). The annotation process is carried out based on guidelines put forward by PARSEME, a multilingual project for more than 26 languages. The corpus contains 4749 VMWEs in about 7500 sentences taken from the Prague Arabic Dependency Treebank. The results notably show a high degree of discontinuity in Arabic VMWEs in comparison to other languages in the PARSEME suite. We also propose analyses of interesting and challenging phenomena encountered during the annotation process. Moreover, we offer the first benchmark for the VMWE identification task in Arabic, by training two state-of-the-art systems, on our Arabic data.", "citations": 0}
{"title": "BERT(s) to Detect Multiword Expressions", "year": 2022, "authors": "Damith Premasiri, Tharindu Ranasinghe", "url": "https://api.semanticscholar.org/CorpusId:251594556", "relevance": 1, "abstract": "Multiword expressions (MWEs) present groups of words in which the meaning of the whole is not derived from the meaning of its parts. The task of processing MWEs is crucial in many natural language processing (NLP) applications, including machine translation and terminology extraction. Therefore, detecting MWEs is a popular research theme. In this paper, we explore state-of-the-art neural transformers in the task of detecting MWEs.We empirically evaluate several transformer models in the dataset for SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM). We show that transformer models outperform the previous neural models based on long short-term memory (LSTM). The code and pre-trained model will be made freely available to the community.", "citations": 9}
{"title": "Token-level Identification of Multiword Expressions using Pre-trained Multilingual Language Models", "year": 2023, "authors": "Raghuraman Swaminathan, Paul Cook", "url": "https://api.semanticscholar.org/CorpusId:258486881", "relevance": 1, "abstract": "In this paper, we consider novel cross-lingual settings for multiword expression (MWE) identification (Ramisch et al., 2020) and idiomaticity prediction (Tayyar Madabushi et al., 2022) in which systems are tested on languages that are unseen during training. Our findings indicate that pre-trained multilingual language models are able to learn knowledge about MWEs and idiomaticity that is not languagespecific. Moreover, we find that training data from other languages can be leveraged to give improvements over monolingual models.", "citations": 3}
{"title": "Deep Neural Representations for Multiword Expressions Detection", "year": 2022, "authors": "Kamil Kanclerz, Maciej Piasecki", "url": "https://api.semanticscholar.org/CorpusId:248780008", "relevance": 1, "abstract": "Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing. Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus, while traditional methods use statistical measures. In our approach, we want to integrate the concepts of those two approaches. We present a novel weakly supervised multiword expressions extraction method which focuses on their behaviour in various contexts. Our method uses a lexicon of English multiword lexical units acquired from The Oxford Dictionary of English as a reference knowledge base and leverages neural language modelling with deep learning architectures. In our approach, we do not need a corpus annotated specifically for the task. The only required components are: a lexicon of multiword units, a large corpus, and a general contextual embeddings model. We propose a method for building a silver dataset by spotting multiword expression occurrences and acquiring statistical collocations as negative samples. Sample representation has been inspired by representations used in Natural Language Inference and relation recognition. Very good results (F1=0.8) were obtained with CNN network applied to individual occurrences followed by weighted voting used to combine results from the whole corpus.The proposed method can be quite easily applied to other languages.", "citations": 10}
{"title": "Investigating the Effects of MWE Identification in Structural Topic Modelling", "year": 2023, "authors": "D. Kokkinakis, Ricardo Mu\u00f1oz S\u00e1nchez, Sebastianus Bruinsma, Mia-Marie Hammarlin", "url": "https://api.semanticscholar.org/CorpusId:258486885", "relevance": 1, "abstract": "Multiword expressions (MWEs) are common word combinations which exhibit idiosyncrasies in various linguistic levels. For various downstream natural language processing applications and tasks, the identification and discovery of MWEs has been proven to be potentially practical and useful, but still challenging to codify. In this paper we investigate various, relevant to MWE, resources and tools for Swedish, and, within a specific application scenario, namely \u2018vaccine skepticism\u2019, we apply structural topic modelling to investigate whether there are any interpretative advantages of identifying MWEs.", "citations": 1}
{"title": "Detecting Unseen Multiword Expressions in American Sign Language", "year": 2023, "authors": "Lee Kezar, Aryan Shukla", "url": "https://api.semanticscholar.org/CorpusId:263334137", "relevance": 1, "abstract": "Multiword expressions present unique challenges in many translation tasks. In an attempt to ultimately apply a multiword expression detection system to the translation of American Sign Language, we built and tested two systems that apply word embeddings from GloVe to determine whether or not the word embeddings of lexemes can be used to predict whether or not those lexemes compose a multiword expression. It became apparent that word embeddings carry data that can detect non-compositionality with decent accuracy.", "citations": 0}
{"title": "NER4ID at SemEval-2022 Task 2: Named Entity Recognition for Idiomaticity Detection", "year": 2022, "authors": "Simone Tedeschi, Roberto Navigli", "url": "https://api.semanticscholar.org/CorpusId:250391025", "relevance": 1, "abstract": "Idioms are lexically-complex phrases whose meaning cannot be derived by compositionally interpreting their components. Although the automatic identification and understanding of idioms is essential for a wide range of Natural Language Understanding tasks, they are still largely under-investigated.This motivated the organization of the SemEval-2022 Task 2, which is divided into two multilingual subtasks: one about idiomaticity detection, and the other about sentence embeddings. In this work, we focus on the first subtask and propose a Transformer-based dual-encoder architecture to compute the semantic similarity between a potentially-idiomatic expression and its context and, based on this, predict idiomaticity. Then, we show how and to what extent Named Entity Recognition can be exploited to reduce the degree of confusion of idiom identification systems and, therefore, improve performance.Our model achieves 92.1 F1 in the one-shot setting and shows strong robustness towards unseen idioms achieving 77.4 F1 in the zero-shot setting. We release our code at https://github.com/Babelscape/ner4id.", "citations": 4}
{"title": "Comparing Probabilistic, Distributional and Transformer-Based Models on Logical Metonymy Interpretation", "year": 2020, "authors": "Giulia Rambelli, Emmanuele Chersoni, Alessandro Lenci, P. Blache, Chu-Ren Huang", "url": "https://www.semanticscholar.org/paper/9a19e9e8639e3a7bf7d8ca1209e838f60a7a8e28", "relevance": 1, "abstract": "In linguistics and cognitive science, Logical metonymies are defined as type clashes between an event-selecting verb and an entity-denoting noun (e.g. The editor finished the article), which are typically interpreted by inferring a hidden event (e.g. reading) on the basis of contextual cues. This paper tackles the problem of logical metonymy interpretation, that is, the retrieval of the covert event via computational methods. We compare different types of models, including the probabilistic and the distributional ones previously introduced in the literature on the topic. For the first time, we also tested on this task some of the recent Transformer-based models, such as BERT, RoBERTa, XLNet, and GPT-2. Our results show a complex scenario, in which the best Transformer-based models and some traditional distributional models perform very similarly. However, the low performance on some of the testing datasets suggests that logical metonymy is still a challenging phenomenon for computational modeling.", "citations": 13}
{"title": "A French corpus annotated for multiword expressions and named entities", "year": 2021, "authors": "Marie Candito, M. Constant, Carlos Ramisch, Agata Savary, Bruno Guillaume, Y. Parmentier, S. Cordeiro", "url": "https://api.semanticscholar.org/CorpusId:229610820", "relevance": 1, "abstract": "\n\n\nWe present the enrichment of a French treebank of various genres with a new annotation layer for multiword expressions (MWEs) and named entities (NEs).1 Our contribution with respect to previous work on NE and MWE annotation is the particular care taken to use formal criteria, organized into decision flowcharts, shedding some light on the interactions between NEs and MWEs. Moreover, in order to cope with the well-known difficulty to draw a clear-cut frontier between compositional expressions and MWEs, we chose to use sufficient criteria only. As a result, annotated MWEs satisfy a varying number of sufficient criteria, accounting for the scalar nature of the MWE status. In addition to the span of the elements, annotation includes the subcategory of NEs (e.g., person, location) and one matching sufficient criterion for non-verbal MWEs (e.g., lexical substitution). The 3,099 sentences of the treebank were double-annotated and adjudicated, and we paid attention to cross-type consistency and compatibility with thesyntactic layer. Overall inter-annotator agreement on non-verbal MWEs and NEs reached 71.1%. The released corpus contains 3,112 annotated NEs and 3,440 MWEs, and is distributed under an open license.\n\n\n", "citations": 10}
{"title": "Verbal Multiword Expression Identification: Do We Need a Sledgehammer to Crack a Nut?", "year": 2020, "authors": "C. Pasquer, Agata Savary, Carlos Ramisch, Jean-Yves Antoine", "url": "https://api.semanticscholar.org/CorpusId:227231875", "relevance": 1, "abstract": "Automatic identification of multiword expressions (MWEs), like \u2018to cut corners\u2019 (to do an incomplete job), is a pre-requisite for semantically-oriented downstream applications. This task is challenging because MWEs, especially verbal ones (VMWEs), exhibit surface variability. This paper deals with a subproblem of VMWE identification: the identification of occurrences of previously seen VMWEs. A simple language-independent system based on a combination of filters competes with the best systems from a recent shared task: it obtains the best averaged F-score over 11 languages (0.6653) and even the best score for both seen and unseen VMWEs due to the high proportion of seen VMWEs in texts. This highlights the fact that focusing on the identification of seen VMWEs could be a strategy to improve VMWE identification in general.", "citations": 22}
{"title": "Discriminative Lexical Semantic Segmentation with Gaps: Running the MWE Gamut", "year": 2014, "authors": "Nathan Schneider, Emily Danchik, Chris Dyer, Noah A. Smith", "url": "https://api.semanticscholar.org/CorpusId:2839348", "relevance": 1, "abstract": "We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) in a sentence, resulting in a lexical semantic segmentation. Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for feature-rich discriminative models. Experiments on a new dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types. Our statistical sequence model greatly outperforms a lookup-based segmentation procedure, achieving nearly 60% F1 for MWE identification.", "citations": 110}
{"title": "Does L2 Proficiency Impact L2-L1 Transfer While Reading L1 Collocations? Evidence From Behavioral and ERP Data", "year": 2021, "authors": "Agnieszka Otwinowska, M. Marecka, A. Casado, Joanna Durlik, J. Szewczyk, Marcin Opacki, Z. Wodniecka", "url": "https://api.semanticscholar.org/CorpusId:238207105", "relevance": 1, "abstract": "Multi-word expressions (MWEs) are fixed, conventional phrases often used by native speakers of a given language (L1). The type of MWEs investigated in this study were collocations. For bilinguals who have intensive contact with the second language (L2), collocational patterns can be transferred from the L2 to the L1 as a result of cross-linguistic influence (CLI). For example, bilingual migrants can accept collocations from their L2 translated to their L1 as correct. In this study, we asked whether such CLI is possible in native speakers living in the L1 environment and whether it depends on their L2 English proficiency. To this end, we created three lists of expressions in Polish: (1) well-formed Polish verb-noun collocations (e.g., ma sens \u2013 \u2217has sense), (2) collocational calques from English (loan translations), where the English verb was replaced by a Polish translation equivalent (e.g., \u2217robi sens \u2013 makes sense), and, as a reference (3) absurd verb-noun expression, where the verb did not collocate with the noun (e.g., \u2217zjada sens \u2013 \u2217eats sense). We embedded the three types of collocations in sentences and presented them to L1 Polish participants of varying L2 English proficiency in two experiments. We investigated whether L2 calques would (1) be explicitly judged as non-native in the L1; (2) whether they would evoke differential brain response than native L1 Polish equivalents in the event-related potentials (ERPs). We also explored whether the sensitivity to CLI in calques depended on participants\u2019 level of proficiency in L2 English. The results indicated that native speakers of Polish assessed the calques from English as less acceptable than the correct Polish collocations. Still, there was no difference in online processing of correct and calques collocations as measured by the ERPs. This suggests a dissociation between explicit offline judgments and indices of online language processing. Interestingly, English L2 proficiency did not modulate these effects. The results indicate that the influence of English on Polish is so pervasive that collocational calques from this language are likely to become accepted and used by Poles.", "citations": 4}
{"title": "Improving Statistical Machine Translation Using Domain Bilingual Multiword Expressions", "year": 2009, "authors": "Zhixiang Ren, Yajuan L\u00fc, Jie Cao, Qun Liu, Yun Huang", "url": "https://api.semanticscholar.org/CorpusId:1130476", "relevance": 1, "abstract": "Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-of-the-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly.", "citations": 114}
{"title": "Pulling their Weight: Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expressions in Context", "year": 2007, "authors": "Paul Cook, A. Fazly, S. Stevenson", "url": "https://api.semanticscholar.org/CorpusId:235425", "relevance": 1, "abstract": "Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (token-based knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques.", "citations": 106}
{"title": "Without lexicons, multiword expression identification will never fly: A position statement", "year": 2019, "authors": "Agata Savary, S. Cordeiro, Carlos Ramisch", "url": "https://api.semanticscholar.org/CorpusId:199379511", "relevance": 1, "abstract": "Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its distributional properties. We also offer a comparative analysis of the state-of-the-art systems, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs\u2019 behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements.", "citations": 26}
{"title": "Learning about phraseology from corpora: A linguistically motivated approach for Multiword Expression identification", "year": 2020, "authors": "U. I\u00f1urrieta, I. Aduriz, Arantza D\u00edaz de Ilarraza, Gorka Labaka, K. Sarasola", "url": "https://api.semanticscholar.org/CorpusId:221357554", "relevance": 1, "abstract": "Multiword Expressions (MWEs) are idiosyncratic combinations of words which pose important challenges to Natural Language Processing. Some kinds of MWEs, such as verbal ones, are particularly hard to identify in corpora, due to their high degree of morphosyntactic flexibility. This paper describes a linguistically motivated method to gather detailed information about verb+noun MWEs (VNMWEs) from corpora. Although the main focus of this study is Spanish, the method is easily adaptable to other languages. Monolingual and parallel corpora are used as input, and data about the morphosyntactic variability of VNMWEs is extracted. This information is then tested in an identification task, obtaining an F score of 0.52, which is considerably higher than related work.", "citations": 7}
{"title": "To Be or Not To Be a Verbal Multiword Expression: A Quest for Discriminating Features", "year": 2020, "authors": "C. Pasquer, Agata Savary, Jean-Yves Antoine, Carlos Ramisch, Nicolas Labroche, Arnaud Giacometti University of Tours, France., A. Univ., Universit'e de Toulon, Cnrs, Lis, Marseille", "url": "https://api.semanticscholar.org/CorpusId:220686421", "relevance": 1, "abstract": "Automatic identification of mutiword expressions (MWEs) is a pre-requisite for semantically-oriented downstream applications. This task is challenging because MWEs, especially verbal ones (VMWEs), exhibit surface variability. However, this variability is usually more restricted than in regular (non-VMWE) constructions, which leads to various variability profiles. We use this fact to determine the optimal set of features which could be used in a supervised classification setting to solve a subproblem of VMWE identification: the identification of occurrences of previously seen VMWEs. Surprisingly, a simple custom frequency-based feature selection method proves more efficient than other standard methods such as Chi-squared test, information gain or decision trees. An SVM classifier using the optimal set of only 6 features outperforms the best systems from a recent shared task on the French seen data.", "citations": 1}
{"title": "Representations in Simple Recurrent Networks Which are Always Compositional", "year": 2004, "authors": "D. Landy", "url": "https://www.semanticscholar.org/paper/f2d3d295efa80275f8afd29ba84f0b128ffb18c4", "relevance": 1, "abstract": "", "citations": 0}
{"title": "Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora", "year": 1998, "authors": "\u00c9ric Gaussier", "url": "https://api.semanticscholar.org/CorpusId:5539038", "relevance": 1, "abstract": "This paper presents a new model for word alignments between parallel sentences, which allows one to accurately estimate different parameters, in a computationally efficient way. An application of this model to bilingual terminology extraction, where terms are identified in one language and gussed, through the alignment process, in the other one, is also described. An experiment conducted on a small English-French parallel corpus gave results with high precision, demonstrating the validity of the model.", "citations": 69}
{"title": "Integrating Morphology with Multi-word Expression Processing in Turkish", "year": 2004, "authors": "Kemal Oflazer, Ozlem Cetinoglu, Bilge Say", "url": "https://api.semanticscholar.org/CorpusId:17107577", "relevance": 1, "abstract": "This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications. In addition to the fairly standard set of lexicalized collocations and multi-word expressions such as named-entities, Turkish uses a quite wide range of semi-lexicalized and non-lexicalized collocations. After an overview of relevant aspects of Turkish, we present a description of the multi-word expressions we handle. We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction. We finally present results from runs over a large corpus and a small gold-standard corpus.", "citations": 58}
{"title": "Confirming the Non-compositionality of Idioms for Sentiment Analysis", "year": 2019, "authors": "Alyssa Hwang, Christopher Hidey", "url": "https://api.semanticscholar.org/CorpusId:199379509", "relevance": 1, "abstract": "An idiom is defined as a non-compositional multiword expression, one whose meaning cannot be deduced from the definitions of the component words. This definition does not explicitly define the compositionality of an idiom\u2019s sentiment; this paper aims to determine whether the sentiment of the component words of an idiom is related to the sentiment of that idiom. We use the Dictionary of Affect in Language augmented by WordNet to give each idiom in the Sentiment Lexicon of IDiomatic Expressions (SLIDE) a component-wise sentiment score and compare it to the phrase-level sentiment label crowdsourced by the creators of SLIDE. We find that there is no discernible relation between these two measures of idiom sentiment. This supports the hypothesis that idioms are not compositional for sentiment along with semantics and motivates further work in handling idioms for sentiment analysis.", "citations": 6}
{"title": "PARSEME-It: an Italian corpus annotated with verbal multiword expressions", "year": 2019, "authors": "J. Monti, Maria Pia di\u00a0Buono", "url": "https://api.semanticscholar.org/CorpusId:241678124", "relevance": 1, "abstract": "The paper describes the PARSEME-It corpus, developed within the PARSEME-It project which aims at the development of methods, tools and resources for multiword expressions (MWE) processing for the Italian language. The project is a spin-off of a larger multilingual project for more than 20 languages from several language families, namely the PARSEME COST Action. The \ufb01rst phase of the project was devoted to verbal multiword expressions (VMWEs). They are a particularly interesting lexical phenomenon because of frequent discontinuity and long-distance dependency. Besides they are very challenging for deep parsing and other Natural Language Processing (NLP) tasks. Notably, MWEs are pervasive in natural languages but are particularly dif\ufb01cult to be handled by NLP tools because of their characteristics and idiomaticity. They pose many challenges to their correct identi\ufb01cation and processing: they are a linguistic phenomenon on the edge between lexicon and grammar, their meaning is not simply the addition of the meanings of the single constituents of the MWEs and they are ambiguous since in several cases their reading can be literal or idiomatic. Although several studies have been devoted to this topic, to the best of our knowledge, our study is the \ufb01rst attempt to provide a general framework for the identi\ufb01cation of VMWEs in running texts and a comprehensive corpus for the Italian language.", "citations": 2}
{"title": "On the Creation and Use of English Compound Nouns.", "year": 1977, "authors": "Pamela A. Downing", "url": "https://www.semanticscholar.org/paper/ed0fcdaf9147e79732aa10f3bda650612997fb2f", "relevance": 1, "abstract": "", "citations": 781}
{"title": "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching", "year": 2025, "authors": "Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen", "url": "https://www.semanticscholar.org/paper/2cf004c498e968a58758139642de8e91b7b6f7e3", "relevance": 1, "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.", "citations": 0}
{"title": "Human Associations Help to Detect Conventionalized Multiword Expressions", "year": 2017, "authors": "Natalia V. Loukachevitch, A. Gerasimova", "url": "https://api.semanticscholar.org/CorpusId:22510714", "relevance": 1, "abstract": "In this paper we show that if we want to obtain human evidence about conventionalization of some phrases, we should ask native speakers about associations they have to a given phrase and its component words. We have shown that if component words of a phrase have each other as frequent associations, then this phrase can be considered as conventionalized. Another type of conventionalized phrases can be revealed using two factors: low entropy of phrase associations and low intersection of component word and phrase associations. The association experiments were performed for the Russian language.", "citations": 2}
{"title": "A Word Embedding Approach to Identifying Verb-Noun Idiomatic Combinations", "year": 2016, "authors": "W. Gharbieh, V. Bhavsar, Paul Cook", "url": "https://api.semanticscholar.org/CorpusId:15797562", "relevance": 1, "abstract": "Verb\u2013noun idiomatic combinations (VNICs) are idioms consisting of a verb with a noun in its direct object position. Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively.", "citations": 27}
{"title": "A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation", "year": 2010, "authors": "S. Tratz, E. Hovy", "url": "https://www.semanticscholar.org/paper/35afa910207445caa3a509a23d41117e7595f262", "relevance": 1, "abstract": "", "citations": 129}
{"title": "Object-oriented lexical encoding of multiword expressions: Short and sweet", "year": 2018, "authors": "Agata Savary, Simon Petitjean, Timm Lichte, Laura Kallmeyer, J. Waszczuk", "url": "https://api.semanticscholar.org/CorpusId:53039844", "relevance": 1, "abstract": "Multiword expressions (MWEs) exhibit both regular and idiosyncratic properties. Their idiosyncrasy requires lexical encoding in parallel with their component words. Their (at times intricate) regularity, on the other hand, calls for means of flexible factorization to avoid redundant descriptions of shared properties. However, so far, non-redundant general-purpose lexical encoding of MWEs has not received a satisfactory solution. We offer a proof of concept that this challenge might be effectively addressed within eXtensible MetaGrammar (XMG), an object-oriented metagrammar framework. We first make an existing metagrammatical resource, the FrenchTAG grammar, MWE-aware. We then evaluate the factorization gain during incremental implementation with XMG on a dataset extracted from an MWE-annotated reference corpus.", "citations": 3}
{"title": "Statistically-Driven Alignment-Based Multiword Expression Identification for Technical Domains", "year": 2009, "authors": "Helena de Medeiros Caseli, Aline Villavicencio, A. Machado, M. J. Finatto", "url": "https://api.semanticscholar.org/CorpusId:262045492", "relevance": 1, "abstract": "Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems. Particularly, the lack of coverage of MWEs in resources can impact negatively on the performance of tasks and applications, and can lead to loss of information or communication errors. This is especially problematic in technical domains, where a significant portion of the vocabulary is composed of MWEs. This paper investigates the use of a statistically-driven alignment-based approach to the identification of MWEs in technical corpora. We look at the use of several sources of data, including parallel corpora, using English and Portuguese data from a corpus of Pediatrics, and examining how a second language can provide relevant cues for this tasks. We report results obtained by a combination of statistical measures and linguistic information, and compare these to the reported in the literature. Such an approach to the (semi-)automatic identification of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates.", "citations": 34}
{"title": "A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions", "year": 2009, "authors": "Linlin Li, C. Sporleder", "url": "https://api.semanticscholar.org/CorpusId:10261581", "relevance": 1, "abstract": "We present a graph-based model for representing the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions.", "citations": 10}
{"title": "A data-driven approach to verbal multiword expression detection. PARSEME Shared Task system description paper", "year": 2017, "authors": "Tiberiu Boros, Sonia Pipa, V. Mititelu, D. Tufi\u0219", "url": "https://api.semanticscholar.org/CorpusId:7453556", "relevance": 1, "abstract": "\u201cMultiword expressions\u201d are groups of words acting as a morphologic, syntactic and semantic unit in linguistic analysis. Verbal multiword expressions represent the subgroup of multiword expressions, namely that in which a verb is the syntactic head of the group considered in its canonical (or dictionary) form. All multiword expressions are a great challenge for natural language processing, but the verbal ones are particularly interesting for tasks such as parsing, as the verb is the central element in the syntactic organization of a sentence. In this paper we introduce our data-driven approach to verbal multiword expressions which was objectively validated during the PARSEME shared task on verbal multiword expressions identification. We tested our approach on 12 languages, and we provide detailed information about corpora composition, feature selection process, validation procedure and performance on all languages.", "citations": 15}
{"title": "How to Account for Idiomatic German Support Verb Constructions in Statistical Machine Translation", "year": 2015, "authors": "Fabienne Cap, M. Nirmal, Marion Weller, Sabine Schulte im Walde", "url": "https://api.semanticscholar.org/CorpusId:5195293", "relevance": 1, "abstract": "Support-verb constructions (i.e., multiword expressions combining a semantically light verb with a predicative noun) are problematic for standard statistical machine translation systems, because SMT systems cannot distinguish between literal and idiomatic uses of the verb. We work on the German to English translation direction, for which the identification of support-verb constructions is challenging due to the relatively free word order of German. We show that we achieve improved translation quality for verb-object supportverb constructions by marking the verbs when occuring in such constructions. Additional evaluations revealed that our systems produce more correct verb translations than a contrastive baseline system without verb markup.", "citations": 25}
{"title": "Combining Linguistic Features for the Detection of Croatian Multiword Expressions", "year": 2017, "authors": "Maja Buljan, J. \u0160najder", "url": "https://api.semanticscholar.org/CorpusId:18469181", "relevance": 1, "abstract": "As multiword expressions (MWEs) exhibit a range of idiosyncrasies, their automatic detection warrants the use of many different features. Tsvetkov and Wintner (2014) proposed a Bayesian network model that combines linguistically motivated features and also models their interactions. In this paper, we extend their model with new features and apply it to Croatian, a morphologically complex and a relatively free word order language, achieving a satisfactory performance of 0.823 F1-score. Furthermore, by comparing against (semi)naive Bayes models, we demonstrate that manually modeling feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available.", "citations": 3}
{"title": "Beyond Words: Deep Learning for Multiword Expressions and Collocations", "year": 2017, "authors": "Valia Kordoni", "url": "https://api.semanticscholar.org/CorpusId:36323193", "relevance": 1, "abstract": "Deep learning has recently shown much promise for NLP applications. Traditionally, in most NLP approaches, documents or sentences are represented by a sparse bag-of-words representation. There is now a lot of work which goes beyond this by adopting a distributed representation of words, by constructing a so-called ``neural embedding'' or vector space representation of each word or document. The aim of this tutorial is to go beyond the learning of word vectors and present methods for learning vector representations for Multiword Expressions and bilingual phrase pairs, all of which are useful for various NLP applications. This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of Multiword Expressions (MWEs), their relevance for the intersection of deep learning and natural language processing, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in machine learning, parsing (syntactic and semantic) and language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.", "citations": 1}
{"title": "Analogy perception applied to seven tests of word comprehension", "year": 2011, "authors": "Peter D. Turney", "url": "https://api.semanticscholar.org/CorpusId:14042683", "relevance": 1, "abstract": "It has been argued that analogy is the core of cognition. In AI research, algorithms for analogy are often limited by the need for hand-coded high-level representations as input. An alternative approach is to use high-level perception, in which high-level representations are automatically generated from raw data. Analogy perception is the process of recognising analogies using high-level perception. We present PairClass, an algorithm for analogy perception that recognises lexical proportional analogies using representations that are automatically generated from a large corpus of raw textual data. A proportional analogy is an analogy of the form A\u2009:\u2009B\u2009::\u2009C\u2009:\u2009D, meaning \u2018A is to B as C is to D\u2019. A lexical proportional analogy is a proportional analogy with words, such as carpenter\u2009:\u2009wood\u2009::\u2009mason\u2009:\u2009stone. PairClass represents the semantic relations between two words using a high-dimensional feature vector, in which the elements are based on frequencies of patterns in the corpus. PairClass recognises analogies by applying standard supervised machine-learning techniques to the feature vectors. We show how seven different tests of word comprehension can be framed as problems of analogy perception and then apply PairClass to the seven resulting sets of analogy perception problems. We achieve competitive results on all seven tests. This is the first time a uniform approach has handled such a range of tests of word comprehension.", "citations": 20}
{"title": "Machine Translation of Non-Contiguous Multiword Units", "year": 2016, "authors": "Anabela Marques Barreiro, Fernando Batista", "url": "https://api.semanticscholar.org/CorpusId:1082179", "relevance": 1, "abstract": "Non-adjacent linguistic phenomena such as non-contiguous multiwords and other phrasal units containing insertions, i.e., words that are not part of the unit, are dif\ufb01cult to process and remain a problem for NLP applications. Non-contiguous multiword units are common across languages and constitute some of the most important challenges to high quality machine translation. This paper presents an empirical analysis of non-contiguous multi-words, and highlights our use of the Logos Model and the Semtab function to deploy semantic knowledge to align non-contiguous multiword units with the goal to translate these units with high \ufb01delity. The phrase level manual alignments illustrated in the paper were produced with the CLUE-Aligner, a Cross-Language Unit Elicitation alignment tool.", "citations": 8}
{"title": "Multilingual Multiword Expressions", "year": 2016, "authors": "Lahari Poddar", "url": "https://api.semanticscholar.org/CorpusId:1197302", "relevance": 1, "abstract": "The project aims to provide a semi-supervised approach to identify Multiword Expressions in a multilingual context consisting of English and most of the major Indian languages. Multiword expressions are a group of words which refers to some conventional or regional way of saying things. If they are literally translated from one language to another the expression will lose its inherent meaning. \nTo automatically extract multiword expressions from a corpus, an extraction pipeline have been constructed which consist of a combination of rule based and statistical approaches. There are several types of multiword expressions which differ from each other widely by construction. We employ different methods to detect different types of multiword expressions. Given a POS tagged corpus in English or any Indian language the system initially applies some regular expression filters to narrow down the search space to certain patterns (like, reduplication, partial reduplication, compound nouns, compound verbs, conjunct verbs etc.). The word sequences matching the required pattern are subjected to a series of linguistic tests which include verb filtering, named entity filtering and hyphenation filtering test to exclude false positives. The candidates are then checked for semantic relationships among themselves (using Wordnet). In order to detect partial reduplication we make use of Wordnet as a lexical database as well as a tool for lemmatising. We detect complex predicates by investigating the features of the constituent words. Statistical methods are applied to detect collocations. Finally, lexicographers examine the list of automatically extracted candidates to validate whether they are true multiword expressions or not and add them to the multiword dictionary accordingly.", "citations": 3}
{"title": "Multi-word expressions in textual inference: Much ado about nothing?", "year": 2009, "authors": "M. Marneffe, Sebastian Pad\u00f3, Christopher D. Manning", "url": "https://api.semanticscholar.org/CorpusId:4881779", "relevance": 1, "abstract": "Multi-word expressions (MWE) have seen much attention from the NLP community. In this paper, we investigate their impact on the recognition of textual entailment (RTE). Using the manual Microsoft Research annotations, we first manually count and classify MWEs in RTE data. We find few, most of which are arguably unlikely to cause processing problems. We then consider the impact of MWEs on a current RTE system. We are unable to confirm that entailment recognition suffers from wrongly aligned MWEs. In addition, MWE alignment is difficult to improve, since MWEs are poorly represented in state-of-the-art paraphrase resources, the only available sources for multi-word similarities. We conclude that RTE should concentrate on other phenomena impacting entailment, and that paraphrase knowledge is best understood as capturing general lexico-syntactic variation.", "citations": 13}
{"title": "Modeling the Statistical Idiosyncrasy of Multiword Expressions", "year": 2015, "authors": "Meghdad Farahmand, Joakim Nivre", "url": "https://api.semanticscholar.org/CorpusId:18544971", "relevance": 1, "abstract": "The focus of this work is statistical idiosyncrasy (or collocational weight) as a discriminant property of multiword expressions. We formalize and model this property, compile a 2-class data set of MWE and non-MWE examples, and evaluate our models on this data set. We present a possible empirical implementation of collocational weight and study its effects on identification and extraction of MWEs. Our models prove to be more effective than baselines in identifying noun-noun MWEs.", "citations": 6}
{"title": "Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations", "year": 2018, "authors": "Vered Shwartz, Ido Dagan", "url": "https://www.semanticscholar.org/paper/a1b8da0b0f1d22c99191721661808df0c2d0d92f", "relevance": 1, "abstract": "Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.", "citations": 26}
{"title": "Paraphrasing Verbs for Noun Compound Interpretation", "year": 2019, "authors": "Preslav Nakov", "url": "https://www.semanticscholar.org/paper/8f3fe2c03a7947cab257d8d1c628b737b36043e2", "relevance": 1, "abstract": "An important challenge for the automatic analysis of English written text is the abundance of noun compounds: sequences of nouns acting as a single noun. In our view, their semantics is best characterized by the set of all possible paraphrasing verbs, with associated weights, e.g., malaria mosquito is carry (23), spread (16), cause (12), transmit (9), etc. Using Amazon's Mechanical Turk, we collect paraphrasing verbs for 250 noun-noun compounds previously proposed in the linguistic literature, thus creating a valuable resource for noun compound interpretation. Using these verbs, we further construct a dataset of pairs of sentences representing a special kind of textual entailment task, where a binary decision is to be made about whether an expression involving a verb and two nouns can be transformed into a noun compound, while preserving the sentence meaning.", "citations": 11}
{"title": "Identifying Bengali Multiword Expressions using Semantic Clustering", "year": 2014, "authors": "Tanmoy Chakraborty, Dipankar Das, Sivaji Bandyopadhyay", "url": "https://api.semanticscholar.org/CorpusId:3851450", "relevance": 1, "abstract": "One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models. As a by-product of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali linguistic thesaurus.", "citations": 5}
{"title": "A Practical Classification of Multiword Expressions", "year": 2007, "authors": "R. Moszczynski", "url": "https://api.semanticscholar.org/CorpusId:7829763", "relevance": 1, "abstract": "The paper proposes a methodology for dealing with multiword expressions in natural language processing applications. It provides a practically justified taxonomy of such units, and suggests the ways in which the individual classes can be processed computationally. While the study is currently limited to Polish and English, we believe our findings can be successfully employed in the processing of other languages, with emphasis on inflectional ones.", "citations": 7}
{"title": "Lexical Access Preference and Constraint Strategies for Improving Multiword Expression Association within Semantic MT Evaluation", "year": 2014, "authors": "Dekai Wu, Chi-kiu (\u7f85\u81f4\u7ff9) Lo, Markus Saers", "url": "https://api.semanticscholar.org/CorpusId:17174337", "relevance": 1, "abstract": "We examine lexical access preferences and constraints in computing multiword expression associations from the standpoint of a high-impact extrinsic task-based performance measure, namely semantic machine translation evaluation. In automated MT evaluation metrics, machine translations are compared against human reference translations, which are almost never worded exactly the same way except in the most trivial of cases. Because of this, one of the most important factors in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative lexical realizations of the same multiword expressions in semantic role fillers. Our results comparing bag-of-words, maximum alignment, and inversion transduction grammars indicate that cognitively motivated ITGs provide superior lexical access characteristics for multiword expression associations, leading to state-of-the-art improvements in correlation with human adequacy judgments.", "citations": 0}
{"title": "Frequency effects in the processing of lexicalized and novel nominal compounds", "year": 1988, "authors": "H. J. Jaarsveld, Gilbert E. Rattink", "url": "https://www.semanticscholar.org/paper/eeaa5c0082285f2ec18388f9bc4ec7f8886b26e6", "relevance": 1, "abstract": "", "citations": 53}
{"title": "Detecting Multiword Verbs in the English Sublanguage of MEDLINE Abstracts", "year": 2004, "authors": "Chun Xiao, D. R\u00f6sner", "url": "https://api.semanticscholar.org/CorpusId:11109811", "relevance": 1, "abstract": "In this paper, we investigate the multiword verbs in the English sublanguage of MEDLINE abstracts. Based on the integration of the domain-specific named entity knowledge and syntactic as well as statistical information, this work mainly focuses on how to evaluate a proper multiword verb candidate. Our results present a sound balance between the low- and high-frequency multiword verb candidates in the sublanguage corpus. We get a F-measure of 0.753, when tested on a manual sample subset consisting of multiword candidates with both low- and high-frequencies.", "citations": 1}
{"title": "Co-occurrence Contexts for Noun Compound Interpretation", "year": 2007, "authors": "Diarmuid \u00d3 S\u00e9aghdha, Ann A. Copestake", "url": "https://www.semanticscholar.org/paper/7597be32c08891a47126cff073312e6c2d4a73d9", "relevance": 1, "abstract": "Contextual information extracted from corpora is frequently used to model semantic similarity. We discuss distinct classes of context types and compare their effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining word-similarity and relation-similarity kernels further improves SVM classification performance.", "citations": 49}
{"title": "Transfer and Multi-Task Learning for Noun\u2013Noun Compound Interpretation", "year": 2018, "authors": "Murhaf Fares, S. Oepen, Erik Velldal", "url": "https://www.semanticscholar.org/paper/03166b7ef32a7fac19b9562713dd870081da374f", "relevance": 1, "abstract": "In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun\u2013noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.", "citations": 14}
{"title": "Support Vector Machines Applied to the Classification of Semantic Relations in Nominalized Noun Phrases", "year": 2004, "authors": "Roxana Girju, Ana-Maria Giuglea, Marian Olteanu, Ovidiu Fortu, Orest Bolohan, D. Moldovan", "url": "https://www.semanticscholar.org/paper/a29fb85803a5c0bceab96b624ad0af41974b573a", "relevance": 1, "abstract": "The discovery of semantic relations in text plays an important role in many NLP applications. This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases. Nominalizations represent a subclass of NP constructions in which either the head or the modifier noun is derived from a verb while the other noun is an argument of this verb. Especially designed features are extracted automatically and used in a Support Vector Machine learning model. The paper presents preliminary results for the semantic classification of the most representative NP patterns using four distinct learning models.", "citations": 23}
{"title": "A reanalysis of the CARIN theory of conceptual combination.", "year": 2007, "authors": "P. Maguire, Barry Devereux, F. Costello, A. Cater", "url": "https://www.semanticscholar.org/paper/acd8ee7865fe3478cc3df25b47c907c1ee855e7e", "relevance": 1, "abstract": "", "citations": 17}
